{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab2_learnbyself.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM/RLoOp4tIYysS5O8Cd/bs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alecia113/NLP-Lab/blob/main/lab2_learnbyself.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRIcer6wvILs"
      },
      "source": [
        "# **Word2Vec** 请直接粘贴老师代码谢谢"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8UuxxU0P88r",
        "outputId": "5fab836f-6b0d-4ce2-cfa2-c17d32889b3a"
      },
      "source": [
        "#开始import\n",
        "'''\n",
        "import pprint\n",
        "import re\n",
        "\n",
        "from lxml import etree  # for parsing Our XML data  用于解析我们的XML数据。\n",
        "\n",
        "数据存储数据分析\n",
        "可扩展标记语言（标准通用标记语言的子集）是一种简单的数据存储语言。\n",
        "XML文件一般指里面写有可扩展标记语言的文件。 XML:可扩展标记语言,标准通用标记语言的子集,是一种用于标记电子文件使其具有结构性的标记语言。它被设计用来传输和存储数据。 XML,是Extensible Markup Language 的缩写。在.NET框架中XML是非常重要的一部分...”\n",
        "\n",
        "import nltk  # data precessing\n",
        "nltk.download('punkt')  #很固定\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "from gensim.models import Word2Vec  # implementing the word2vec family of algorithms.\n",
        "#用于实现word2vec 系列算法 ms。\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action = 'ignore', category = FutureWarning) \n",
        "\n",
        "'''\n",
        "import pprint\n",
        "import re\n",
        "\n",
        "# For parsing our XML data\n",
        "from lxml import etree \n",
        "\n",
        "# For data processing\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# For implementing the word2vec family of algorithms\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "kih-fDj2Rszh",
        "outputId": "bd909165-2469-41e1-d3a5-30019d21ee4d"
      },
      "source": [
        "#https://www.cnblogs.com/leomei91/p/7660928.html\n",
        "#https://blog.csdn.net/weixin_42214743/article/details/107720074\n",
        "#有些警告是可以忽略的\n",
        "'''\n",
        "调用模型还是调整参数\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "具体函数：\n",
        "warnings.filterwarnings(action, message='', category=Warning, module='', lineno=0, append=False)\n",
        "'''"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n调用模型还是调整参数\\n\\nimport warnings\\nwarnings.filterwarnings('ignore')\\n\\n具体函数：\\nwarnings.filterwarnings(action, message='', category=Warning, module='', lineno=0, append=False)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aznd9pPMSHb1"
      },
      "source": [
        "**然后开始下载数据** use TED script(脚本） from google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjUFH3MfSQsW"
      },
      "source": [
        "# 下面代码是： generate a link +  a field (字段) ---> verification code (enter)\n",
        "#   click link  go to google sign in ; ; (own google account) \n",
        "#  copy  verification code + into field + press enter\n",
        "'''\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth  #GoogleAuthenticator(谷歌身份验证器)\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials  #谷歌  全权证书\n",
        "\n",
        "#Authenticate  [ɔːˈθentɪkeɪt]  v  鉴定，验证（凹三忒忒咳）\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "#看看是不是这个 也不是\n",
        "'''\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9esb_dDkYP0U"
      },
      "source": [
        "下载TED Scripts from GD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCxqrtAGY7wq"
      },
      "source": [
        "#点击files tab + see file is download successfully\n",
        "'''\n",
        "id = '1B47OiEiG2Lo1jUY6hy_zMmHBxfKQuJ8-'\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('ted_en-20160408.xml')\n",
        "'''\n",
        "#在路径中就会过个/content/ted_en-20160408.xml\n",
        "#好像是这个的问题\n",
        "id = '1B47OiEiG2Lo1jUY6hy_zMmHBxfKQuJ8-'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('ted_en-20160408.xml')  \n",
        "\n",
        "#尴尬看起来不是这个的问题"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqybtF6HY3R_"
      },
      "source": [
        "**数据处理 data preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "2U1sBHRiaGWR",
        "outputId": "c1456a20-ad0a-49a2-9c95-fa35de430e87"
      },
      "source": [
        "targetXML=open('ted_en-20160408.xml', 'r', encoding='UTF8')\n",
        "\n",
        "# Getting contents of <content> tag from the xml file\n",
        "target_text = etree.parse(targetXML)\n",
        "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n",
        "\n",
        "# Removing \"Sound-effect labels\" using regular expression (regex) (i.e. (Audio), (Laughter))\n",
        "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
        "\n",
        "# Tokenising the sentence to process it by using NLTK library\n",
        "sent_text=sent_tokenize(content_text)\n",
        "\n",
        "# Removing punctuation and changing all characters to lower case\n",
        "normalized_text = []\n",
        "for string in sent_text:\n",
        "     tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
        "     normalized_text.append(tokens)\n",
        "\n",
        "# Tokenising each sentence to process individual word\n",
        "sentences=[]\n",
        "sentences=[word_tokenize(sentence) for sentence in normalized_text]\n",
        "\n",
        "# Prints only 10 (tokenised) sentences\n",
        "print(sentences[:10])\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "targetXML =open('ted_en-20160408.xml','r',encoding = 'UTF8') #等号后面必须要直接跟open；用utff8来打开ted文件\n",
        "#open(file, mode='r', buffering=-1, encoding=None, errors=None, newline=None, closefd=True, opener=None)\n",
        "\n",
        "target_text = etree.parse(targetXML)#etree.parse直接接受一个文档，按照文档结构解析（本地文件）\n",
        "'''\n",
        "'''\n",
        "#etree.parse(*args, **kwargs)\n",
        "#parse(source, parser=None, base_url=None)\n",
        "'''\n",
        "'''\n",
        "#etree.html可以解析html文件：（服务器上返回的html数据）\n",
        "#https://blog.csdn.net/cbiexi/article/details/104479744?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522161568688316780255255960%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=161568688316780255255960&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-3-104479744.first_rank_v2_pc_rank_v29&utm_term=etree.parse\n",
        "#https://blog.csdn.net/myself029/article/details/79954301?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522161568688316780255255960%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=161568688316780255255960&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-2-79954301.first_rank_v2_pc_rank_v29&utm_term=etree.parse\n",
        "#etree 更像是展示出来这个文本文档等等\n",
        "parse_text = '\\n'.join(target_text.xpath('//content/text()')) #获取<内容>标记的内容从xml 文件中【par解析文字】\n",
        "# 从xml文件中获取<content>标签的内容。\n",
        "# 【在爬取内容】\"\\n\".join() 来对字符串列表进行处理，不会出现不连贯情况\n",
        "#https://blog.csdn.net/freeking101/article/details/64461574?utm_medium=distribute.pc_relevant.none-task-blog-OPENSEARCH-1.control&dist_request_id=&depth_1-utm_source=distribute.pc_relevant.none-task-blog-OPENSEARCH-1.control\n",
        "#https://blog.csdn.net/weixin_39751871/article/details/111537728?ops_request_misc=&request_id=&biz_id=102&utm_term=%27%5Cn%27.join(target_text.xpath(%27/&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-0-111537728.first_rank_v2_pc_rank_v29\n",
        "\n",
        "\n",
        "content_text = re.sub(r'\\([^)]*\\)','',parse_text) # 删除音效标签（音频，笑声）#获得文本\n",
        "'''\n",
        "'''\n",
        "\\: 将下一个字符————》特殊字符、原义字符。[eg]:\\( ==(\n",
        "^ : 匹配输入字符串的开始位置\n",
        "*: 匹配前面的子表达式0次或者多次 * == {0,}\n",
        "'''\n",
        "\n",
        "\n",
        "'''\n",
        "#regex 检索词、、正则表达式\n",
        "#正则表达式强大的文本匹配功能。很多文本匹配处理，如果没有正则表达式，还真的很难做出来。\n",
        "#https://blog.csdn.net/wyb880501/article/details/79813658\n",
        "\n",
        "sent_text = sent_tokenize(content_text) #利用NLTK库 对句子进行标记化处理。# 将句子分开。sent_tokenize\n",
        "#https://blog.csdn.net/wyb880501/article/details/79813658\n",
        "#regex(蕊·寨·课·嘶)n\n",
        "#https://blog.csdn.net/Wisimer/article/details/89556404?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522161569572316780271589227%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=161569572316780271589227&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-1-89556404.first_rank_v2_pc_rank_v29&utm_term=NLTK\n",
        "#NLTK 的讲解  \n",
        "\n",
        "#remove punctuation + change all characters --> lower case  去掉标点符号并将所有字符改为小写。\n",
        "normalized_text = []     #归一化文本  normalized (孬·某·赖·子·的)\n",
        " #从你的语句中获得单词\n",
        "for string in sent_text: \n",
        "  tokens = re.sub(r\"[^a-z0-9]+\",\" \",string.lower()) #[^a-z]负值字符范围;匹配任何不在指定范围内的任意字符;;匹配任何不在 ‘a' 到 ‘z' 范围内的任意字符。 标点符号。\n",
        "  normalized_text.append(tokens) # 把你处理好的东西丢到列表里。  #这个地方还是句子\n",
        "#tokenize （偷·可·耐·滋）\n",
        "# Tokenising (偷·可·耐·滋鹰）符号化 each sentence __process individual word #对每个句子进行符号化处理，以处理单个单词\n",
        "sentences=[]\n",
        "sentences=[word_tokenize(sentence) for sentence in normalized_text] #从文本冲取出来然后拆词\n",
        "\n",
        "#prints only 10 tokenised sentences  \n",
        "print(sentences[:10])\n",
        "\n",
        "#这个也不是\n",
        "'''"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new'], ['to', 'me', 'the', 'real', 'real', 'solution', 'to', 'quality', 'growth', 'is', 'figuring', 'out', 'the', 'balance', 'between', 'two', 'activities', 'exploration', 'and', 'exploitation'], ['both', 'are', 'necessary', 'but', 'it', 'can', 'be', 'too', 'much', 'of', 'a', 'good', 'thing'], ['consider', 'facit'], ['i', 'm', 'actually', 'old', 'enough', 'to', 'remember', 'them'], ['facit', 'was', 'a', 'fantastic', 'company'], ['they', 'were', 'born', 'deep', 'in', 'the', 'swedish', 'forest', 'and', 'they', 'made', 'the', 'best', 'mechanical', 'calculators', 'in', 'the', 'world'], ['everybody', 'used', 'them'], ['and', 'what', 'did', 'facit', 'do', 'when', 'the', 'electronic', 'calculator', 'came', 'along'], ['they', 'continued', 'doing', 'exactly', 'the', 'same']]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n#regex 检索词、、正则表达式\\n#正则表达式强大的文本匹配功能。很多文本匹配处理，如果没有正则表达式，还真的很难做出来。\\n#https://blog.csdn.net/wyb880501/article/details/79813658\\n\\nsent_text = sent_tokenize(content_text) #利用NLTK库 对句子进行标记化处理。# 将句子分开。sent_tokenize\\n#https://blog.csdn.net/wyb880501/article/details/79813658\\n#regex(蕊·寨·课·嘶)n\\n#https://blog.csdn.net/Wisimer/article/details/89556404?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522161569572316780271589227%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=161569572316780271589227&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-1-89556404.first_rank_v2_pc_rank_v29&utm_term=NLTK\\n#NLTK 的讲解  \\n\\n#remove punctuation + change all characters --> lower case  去掉标点符号并将所有字符改为小写。\\nnormalized_text = []     #归一化文本  normalized (孬·某·赖·子·的)\\n #从你的语句中获得单词\\nfor string in sent_text: \\n  tokens = re.sub(r\"[^a-z0-9]+\",\" \",string.lower()) #[^a-z]负值字符范围;匹配任何不在指定范围内的任意字符;;匹配任何不在 ‘a\\' 到 ‘z\\' 范围内的任意字符。 标点符号。\\n  normalized_text.append(tokens) # 把你处理好的东西丢到列表里。  #这个地方还是句子\\n#tokenize （偷·可·耐·滋）\\n# Tokenising (偷·可·耐·滋鹰）符号化 each sentence __process individual word #对每个句子进行符号化处理，以处理单个单词\\nsentences=[]\\nsentences=[word_tokenize(sentence) for sentence in normalized_text] #从文本冲取出来然后拆词\\n\\n#prints only 10 tokenised sentences  \\nprint(sentences[:10])\\n\\n#这个也不是\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQQJOG8EGnEW"
      },
      "source": [
        "# **CBOW ——continuous bag of Words** Word2Vec 连续单词袋"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wjq5JioaG1lN"
      },
      "source": [
        "有关gensim.models.word2vec的更多详细信息，您可以参考Gensim Word2Vec的API。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "dCxH6Mb9GwYt",
        "outputId": "5afad1eb-6be9-4549-9e72-54e854a13f0c"
      },
      "source": [
        "#有关gensim.models.word2vec的更多详细信息，您可以参考Gensim Word2Vec的API。\n",
        "#https://radimrehurek.com/gensim/models/word2vec.html\n",
        "\n",
        "wv_cbow_model = Word2Vec(sentences=sentences, size=100, window=5, min_count=5 , workers=2, sg=0)\n",
        "\n",
        "'''\n",
        "Word2Vec(sentences=None, corpus_file=None, size=100, alpha=0.025, window=5, \n",
        "min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, \n",
        "min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, \n",
        "hashfxn=<built-in function hash>, iter=5, null_word=0, trim_rule=None, \n",
        "sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), max_final_vocab=None)\n",
        "'''\n",
        "'''\n",
        " Initialize and train a word2vec model with the following parameters:\n",
        " 初始化+训练w2v模型\n",
        "# sentence: iterable of iterables, i.e. the list of lists of tokens from our data\n",
        "可迭代的迭代： 符号化来自我们数据的列表们的列表\n",
        "# size: dimensionality of the word vectors\n",
        "单词向量的维度\n",
        "# window: window size\n",
        "# min_count: ignores all words with total frequency lower than the specified count value\n",
        "频率比具体化数值低的单词都忽略。 （忽略总频率低语指定计数值的所有单词）\n",
        "\n",
        "# workers: Use specified number of worker threads （线程；主题）【嘶·若案·子】 to train the model (=faster training with multicore machines)\n",
        "\n",
        "运行程序就是进程 进程中最小单位是一个线程； 一个进程有至少一个线程\n",
        "清理垃圾之后再杀毒 是有顺序的，~~单线程 \n",
        "#https://blog.csdn.net/csdnnews/article/details/82321777?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522161569868516780264066483%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=161569868516780264066483&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-3-82321777.first_rank_v2_pc_rank_v29&utm_term=线程\n",
        "还讲了线程安全\n",
        "\n",
        "【worker是什么含义每天搞懂】使用指定数量的worker 线程来训练模型（=使用multicore机器进行更快的训练）\n",
        "\n",
        "# sg: training algorithm, 0 for CBOW, 1 for skip-gram\n",
        "sg：训练算法，CBOW 为0 ； Skip-gram 为1\n",
        "'''\n",
        "#这个地方空格了后面的地方没空格"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n Initialize and train a word2vec model with the following parameters:\\n 初始化+训练w2v模型\\n# sentence: iterable of iterables, i.e. the list of lists of tokens from our data\\n可迭代的迭代： 符号化来自我们数据的列表们的列表\\n# size: dimensionality of the word vectors\\n单词向量的维度\\n# window: window size\\n# min_count: ignores all words with total frequency lower than the specified count value\\n频率比具体化数值低的单词都忽略。 （忽略总频率低语指定计数值的所有单词）\\n\\n# workers: Use specified number of worker threads （线程；主题）【嘶·若案·子】 to train the model (=faster training with multicore machines)\\n\\n运行程序就是进程 进程中最小单位是一个线程； 一个进程有至少一个线程\\n清理垃圾之后再杀毒 是有顺序的，~~单线程 \\n#https://blog.csdn.net/csdnnews/article/details/82321777?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522161569868516780264066483%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=161569868516780264066483&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-3-82321777.first_rank_v2_pc_rank_v29&utm_term=线程\\n还讲了线程安全\\n\\n【worker是什么含义每天搞懂】使用指定数量的worker 线程来训练模型（=使用multicore机器进行更快的训练）\\n\\n# sg: training algorithm, 0 for CBOW, 1 for skip-gram\\nsg：训练算法，CBOW 为0 ； Skip-gram 为1\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeZClxBQPEqm",
        "outputId": "17d07271-1647-4aac-d59e-fbfcd272a247"
      },
      "source": [
        "similar_words = wv_cbow_model.wv.most_similar(\"man\") #topn = 10 default (的A·佛奥·忒) 默认值\n",
        "pprint.pprint(similar_words)\n",
        "# The trained word vectors are stored in a KeyedVectors instance（实例） as model.wv\n",
        "#训练单词向量存在KeyedVectors 实例作为model.wv ([主宾谓]训练后的词向量以model.wv的形式存在keyedvec实例中)\n",
        "# Get the top 10 similar words to 'man' by calling most_similar() \n",
        "#通过calling most_similar() 得到 前十个相似单词。\n",
        "#通过调用most_similar() 获取与\"man\"相似的前十个词\n",
        "# most_similar() computes cosine similarity between a simple mean of the vectors of the given words and the vectors for each word in the model \n",
        "#他是变得。\n",
        "#most_similar() 计算余弦相似度，在一个简单的被给单词的向量含义和每个在模型中向量的单词\n",
        "#（计算给定的单词的向量和模型中每个单词的向量的简单均值之间的（ a simple mean of） 余弦相似度。\n",
        "#【不懂】\n",
        "#【值是变化的可能是获取的文件不够稳定。所有的值都是变化的。】但是频率差距不大。"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('woman', 0.8470235466957092),\n",
            " ('guy', 0.7923800945281982),\n",
            " ('lady', 0.7808774709701538),\n",
            " ('boy', 0.7683364152908325),\n",
            " ('girl', 0.7389755845069885),\n",
            " ('gentleman', 0.7324548363685608),\n",
            " ('soldier', 0.7201880216598511),\n",
            " ('kid', 0.7118163704872131),\n",
            " ('philosopher', 0.6646140217781067),\n",
            " ('photographer', 0.6628036499023438)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "YGmwTT4ywGHI",
        "outputId": "041d4dd7-78b3-4ae4-c0ee-7efceaf6a7a9"
      },
      "source": [
        "'''\n",
        "#sg=0；；cbow \n",
        "wv_cbow_model = Word2Vec(sentences = sentences, size = 100, window = 5, min_count = 5 , workers = 2, sg = 0)\n",
        "similar_words = wv_cbow_model.wv.most_similar(\"man\") \n",
        "\n",
        "#sg=1；； sg 模型\n",
        "wv_sg_model = Word2Vec(sentences=sentences, size=100,window=5,min_count=5,workers=2,sg=1)\n",
        "similar_words=wv_sg_model.wv.most_similar(\"man\") \n",
        "\n",
        "ft_sg_model = FastText(sentences,size=100,window=5,min_count=5,workers=2,sg=1)\n",
        "result=ft_sg_model.wv.most_similar(\"electrofishing\")\n",
        "\n",
        "ft_cbow_model = FastText(sentences,size=100,window=5,min_count=5,workers=2,sg=0)\n",
        "result=ft_cbow_model.wv.most_similar(\"electrofishing\")\n",
        "\n",
        "\n",
        "#就是把（）中的单词变成了K-M+W （）是变化的其他是不变的\n",
        "'''\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n#sg=0；；cbow \\nwv_cbow_model = Word2Vec(sentences = sentences, size = 100, window = 5, min_count = 5 , workers = 2, sg = 0)\\nsimilar_words = wv_cbow_model.wv.most_similar(\"man\") \\n\\n#sg=1；； sg 模型\\nwv_sg_model = Word2Vec(sentences=sentences, size=100,window=5,min_count=5,workers=2,sg=1)\\nsimilar_words=wv_sg_model.wv.most_similar(\"man\") \\n\\nft_sg_model = FastText(sentences,size=100,window=5,min_count=5,workers=2,sg=1)\\nresult=ft_sg_model.wv.most_similar(\"electrofishing\")\\n\\nft_cbow_model = FastText(sentences,size=100,window=5,min_count=5,workers=2,sg=0)\\nresult=ft_cbow_model.wv.most_similar(\"electrofishing\")\\n\\n\\n#就是把（）中的单词变成了K-M+W （）是变化的其他是不变的\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8-uvvr9Tk2Z"
      },
      "source": [
        "# Word2Vec-skip Gram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwFL6nQWTuwM"
      },
      "source": [
        "'''\n",
        "wv_sg_model = Word2Vec(sentences=sentences, size=100, window=5, min_count=5, workers=2, sg=1)\n",
        "# KG 和CBOW不同的是sg这个参数多了 从0变1 这样就从CBOW 模型切换到SG模型了。\n",
        " #跟空格有关系吗？？没关系\n",
        " '''\n",
        " # Now we switch to a Skip Gram model by setting parameter sg=1\n",
        "wv_sg_model = Word2Vec(sentences=sentences, size=100, window=5, min_count=5, workers=2, sg=1)\n",
        "#不是上面是这出了问题 不是这个出了问题 与空格无关系"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yIcEhosXEQ8",
        "outputId": "27702443-d1e3-4157-8a50-3b700d03b2dc"
      },
      "source": [
        "similar_words=wv_sg_model.wv.most_similar(\"man\") #唯一不同把调用的模型名字变了。\n",
        "pprint.pprint(similar_words)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('woman', 0.7677208185195923),\n",
            " ('guy', 0.7676129937171936),\n",
            " ('boy', 0.7084762454032898),\n",
            " ('soldier', 0.7077591419219971),\n",
            " ('gentleman', 0.6993050575256348),\n",
            " ('girl', 0.6862406730651855),\n",
            " ('son', 0.6764705181121826),\n",
            " ('rabbi', 0.6757755279541016),\n",
            " ('michelangelo', 0.6683143973350525),\n",
            " ('imam', 0.662891685962677)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlK01yCVaC3b"
      },
      "source": [
        "# Word2Vec vs FastText"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZOxRJmWaIsC"
      },
      "source": [
        "Word2Vec-Skip Gram 不能发现相似文字去’electrofishing‘ 电子钓鱼；因为电子钓鱼不再词汇表中。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "madbMz6qag_8"
      },
      "source": [
        "similar_words=wv_sg_model.wv.most_similar(\"electrofishing\")\n",
        "pprint.pprint(similar_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-egW_iAaau4G"
      },
      "source": [
        "# FastText-Skip Gram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "016nlVETa9RY"
      },
      "source": [
        "from gensim.models import FastText  #gensim.models 遗传因子模型\n",
        "'''\n",
        "gensim是一款强大的自然语言处理工具，里面包括N多常见模型：\n",
        "\n",
        "基本的语料处理工具\n",
        "LSI\n",
        "LDA\n",
        "HDP\n",
        "DTM\n",
        "DIM\n",
        "TF-IDF\n",
        "word2vec、paragraph2vec\n",
        ".\n",
        "#gensim详解 https://blog.csdn.net/sinat_26917383/article/details/69803018?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522161572013816780262535820%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&request_id=161572013816780262535820&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_v2~rank_v29-6-69803018.first_rank_v2_pc_rank_v29&utm_term=gensim.models+用法\n",
        "【好慢啊……】\n",
        "\n",
        "\n",
        "model=gensim.models.Word2Vec(sentences,sg=1,size=100,window=5,min_count=2,negative=3,sample=0.001,hs=1,workers=4)\n",
        " \n",
        "#该步骤也可分解为以下三步（但没必要）：\n",
        "#model=gensim.model.Word2Vec() 建立一个空的模型对象\n",
        "#model.build_vocab(sentences) 遍历一次语料库建立词典\n",
        "#model.train(sentences) 第二次遍历语料库建立神经网络模型\n",
        " \n",
        "#sg=1是skip—gram算法，对低频词敏感，默认sg=0为CBOW算法\n",
        "#size是神经网络层数，值太大则会耗内存并使算法计算变慢，一般值取为100到200之间。\n",
        "#window是句子中当前词与目标词之间的最大距离，3表示在目标词前看3-b个词，后面看b个词（b在0-3之间随机）\n",
        "#min_count是对词进行过滤，频率小于min-count的单词则会被忽视，默认值为5。\n",
        "#negative和sample可根据训练结果进行微调，sample表示更高频率的词被随机下采样到所设置的阈值，默认值为1e-3,\n",
        "#negative: 如果>0,则会采用negativesamping，用于设置多少个noise words\n",
        "#hs=1表示层级softmax将会被使用，默认hs=0且negative不为0，则负采样将会被选择使用。\n",
        "#workers是线程数，此参数只有在安装了Cpython后才有效，否则只能使用单核\n",
        " \n",
        "# min_count，是去除小于min_count的单词\n",
        "# size，神经网络层数\n",
        "# sg， 算法选择\n",
        "# window， 句子中当前词与目标词之间的最大距离\n",
        "# workers，线程数\n",
        "#model.save(\"文本名\")\t#模型会保存到该 .py文件同级目录下，该模型打开为乱码\n",
        "\n",
        "\n",
        "和上面那个是一个包\n",
        "\n",
        "'''\n",
        "ft_sg_model = FastText(sentences,size=100,window=5,min_count=5,workers=2,sg=1)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "id": "UCQ9a6XdeCWr",
        "outputId": "43305345-fc07-42ab-ebbf-86fc4923fd32"
      },
      "source": [
        "result=ft_sg_model.wv.most_similar(\"electrofishing\") #.wv.most_similar这个是固定的前面的模块不同\n",
        "pprint.pprint(result) #截断的\n",
        "'''\n",
        "skip gram 架构（sg=1) 初始化和训练fasttext\n",
        "fasttext允许我们获取词汇外单词的单词向量\n",
        "'''"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('electrolux', 0.80483078956604),\n",
            " ('electro', 0.7881948947906494),\n",
            " ('electrolyte', 0.7861843109130859),\n",
            " ('airbus', 0.7736728191375732),\n",
            " ('electroshock', 0.7685168981552124),\n",
            " ('airbag', 0.7656456232070923),\n",
            " ('electrochemical', 0.7533538341522217),\n",
            " ('electric', 0.7525812387466431),\n",
            " ('electrons', 0.751227080821991),\n",
            " ('electrogram', 0.7504618763923645)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nskip gram 架构（sg=1) 初始化和训练fasttext\\nfasttext允许我们获取词汇外单词的单词向量\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zE7kVcKZgu1Q"
      },
      "source": [
        "# FastText-CBOW (continuous bag-of-words)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeS-MWuug273"
      },
      "source": [
        "#initialize + train FastText with CBOW architecture(sg=0)\n",
        "#现在我们使用CBOW的构架初始化和训练FastText （sg=0)  \n",
        "ft_cbow_model = FastText(sentences,size=100,window=5,min_count=5,workers=2,sg=0)\n",
        "#[训练依旧很慢]"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0NVH934iC3W",
        "outputId": "4f52dbf8-cea0-4245-d040-f2574eecf909"
      },
      "source": [
        "result=ft_cbow_model.wv.most_similar(\"electrofishing\") #不训练的话；就会：AttributeError: 'FastText' object has no attribute 'wv_most_similar\n",
        "pprint.pprint(result)\n",
        "#这里是wv。most-"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('electric', 0.9249635934829712),\n",
            " ('electro', 0.9110923409461975),\n",
            " ('electrolux', 0.8995359539985657),\n",
            " ('electron', 0.8837720155715942),\n",
            " ('electronic', 0.8823838233947754),\n",
            " ('electrolyte', 0.8793928623199463),\n",
            " ('electrical', 0.8793478012084961),\n",
            " ('electroshock', 0.8777401447296143),\n",
            " ('electromagnet', 0.8753716349601746),\n",
            " ('electrode', 0.8738146424293518)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHjyESOTiU6t"
      },
      "source": [
        "# **King -Man +Woman = ? **"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXDHo7J8iyeX"
      },
      "source": [
        "**Try  both CBOW + SKIP Gram model compute \"k-M +W **"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzyLyJDTzhR0"
      },
      "source": [
        "得到的词的不一样呢"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpdLeVRojEGD",
        "outputId": "758a1621-89bd-4f5f-ea45-7b62cf01a521"
      },
      "source": [
        "# sepcify positive/negative word list ~~~ positive/negative parameters\n",
        "#我们可以使用正/负参数指定正/负词列表\n",
        "# Top N most similar words can be specified with topn parameter\n",
        "#可以使用topn参数是定前N个最相似的单词。\n",
        "result = wv_cbow_model.wv.most_similar(positive = ['woman','king'],negative =['man'],topn=1)\n",
        "#正的就是positive 负的就是negative\n",
        "print(result) #指定前1个最相似的"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('president', 0.7811003923416138)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BF68elivjEPA",
        "outputId": "531152e6-5782-40bd-e70d-777f1570048b"
      },
      "source": [
        "'''\n",
        "result = wv_sg_model.wv.most_similar(positive=['woman','king'],negative=['man'],topn=1)\n",
        "print(result) #wv_sg   #这个地方单词输入很大啊。也不知道怎么了；也不是这句话的问题\n",
        "'''\n",
        "result = wv_sg_model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
        "print(result)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('luther', 0.6677122712135315)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxuwJD3zpQIw",
        "outputId": "5c4fecee-def0-46c6-cea1-c5865d6e677e"
      },
      "source": [
        "result = ft_cbow_model.wv.most_similar(positive=['woman','king'],negative=['man'],topn=1)\n",
        "print(result) #ft_cbow "
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('kidding', 0.8879081010818481)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26rFqrKKp2Fm",
        "outputId": "c74cc9d1-8506-4471-83fa-e7de0da07cd6"
      },
      "source": [
        "result = ft_sg_model.wv.most_similar(positive=['woman','king'],negative=['man'],topn=1)\n",
        "print(result)  #ft_sg  sg"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('pauling', 0.701784610748291)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JN5hTwnIsp5B"
      },
      "source": [
        "没有足够的数据来回答“女王”，下面我们将用更大的数据进行训练。（google 已经使用了google news 数据训练了Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmoHfP2xuPll"
      },
      "source": [
        "# Using Pretrained word embeddings with Gensim ；在Gensim中使用预训练词嵌入"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMPeDkkB1ttj"
      },
      "source": [
        "从google预训练的word2Vec二进制文件下载并加载"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nze_4vGI1aH_"
      },
      "source": [
        "#https://code.google.com/archive/p/word2vec/  #连接到项目\n",
        "# Download the pre-trained vectors trained on part of Google News dataset (about 100 billion words)\n",
        "# Beware, this file is big (3.39GB) - might be long waiting!\n",
        "# 1000亿个单词  下载在google 新闻数据集的一部分中 经过训练的预训练向量。（3.39GB）\n",
        "id2 = '0B7XkCwpI5KDYNlNUTTlSS21pQmM'\n",
        "downloaded = drive.CreateFile({'id':id2})\n",
        "downloaded.GetContentFile('GoogleNews-vectors-negative300.bin.gz') "
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5-fAA8x34s8"
      },
      "source": [
        "#Uncompress the downloaded file 解压下载的文件。\n",
        "! gzip -d /content/GoogleNews-vectors-negative300.bin.gz\n",
        "#zip: /content/GoogleNews-vectors-negative300.bin already exists; do you wish to overwrite (y or n)?\n",
        "#zip。/content/GoogleNews-vectors-negative300.bin已经存在，你要不要覆盖（y或n）？"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dm9Jr0uI4Tpq"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "#load pretrained vectors with keyedvectors instance(example eg) : might be long waiting!\n",
        "filename = 'GoogleNews-vectors-negative300.bin'\n",
        "gn_mv_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
        "#以姿态将KeyedVectors 加载到预训练的向量中-可能要等很久\n",
        "#851-52"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "Yy8gJfiQtB8v",
        "outputId": "b38d2b4d-c83e-44b4-9e69-c01054e68227"
      },
      "source": [
        "# now we can try calculate \"King-man+woman = ?\" again\n",
        "result = gn_mv_model.most_similar(positive=['woman','king'], negative=['man'], topn=1)\n",
        "print(result)\n",
        "#还真是ram不够用中断了\n",
        "#老师[（'queen'，0.7118192911148071）]\n",
        "#这个结果就正确了"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-0fb90f95c677>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# now we can try calculate \"King-man+woman = ?\" again\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgn_mv_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'woman'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'king'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'man'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#还真是ram不够用中断了\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#老师[（'queen'，0.7118192911148071）]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[0mnegative\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_sims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36minit_sims\u001b[0;34m(self, replace)\u001b[0m\n\u001b[1;32m   1364\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1365\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1366\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors_norm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mREAL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2g8bfU7ovbEN"
      },
      "source": [
        "# load a pretained word embedding model using API ; \n",
        "\n",
        "> 使用API加载预训练的词嵌入模型\n",
        "\n",
        "> another way: use Gensim to load pretrained word\n",
        "\n",
        "> try GloVe embedding trained on twitter data尝试对Twitter数据进行训练的GloVe嵌入\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHa052uTwFbq"
      },
      "source": [
        "import gensim.downloader as api\n",
        " \n",
        "model = api.load(\"glove-twitter-25\") #download model + return as object ready for use(下载模型备用；作为对象返回)\n",
        "print(model.similarity(\"cat\",\"dog\"))#similarity function can calculate the cosine similarity between 2 give words.\n",
        "#similarity() 可以计算两个给定单词的余弦相似度\n",
        "#99.3% 104.1/104.8MB downloaded0.95908207   0.040917932987213135 老师的\n",
        "#为什么没加载完呢？\n",
        "#0.95908207\n",
        "print(model.distance(\"cat\",\"dog\"))\n",
        "# a another way calculating the similarity between 2 given words ; returns 1-cosine  similarity instead \n",
        "'''\n",
        "0.95908207\n",
        "0.040917932987213135\n",
        "'''\n",
        "#就算是没加载完结果还是一样。而且会用到RAM将近10个G\n",
        "#第二个是返回1-cosine 相似度\n",
        "#余弦距离 = 1 - 余弦相似度，其取值范围为[ 0， 2 ]，即相同的两个向量余弦距离为0\n",
        "'''\n",
        "欧氏距离体现数值上的绝对差异，而余弦距离体现方向上的相对差异。\n",
        "余弦距离不是严格定义上的距离！\n",
        "https://blog.csdn.net/m0_38024592/article/details/100075613?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522161575999916780265455768%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=161575999916780265455768&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~baidu_landing_v2~default-1-100075613.first_rank_v2_pc_rank_v29&utm_term=1-cosine+\n",
        "\n",
        "\n",
        "'''\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yutG8CmTz4XQ"
      },
      "source": [
        "# 【tips】 paly with colab form fields 玩colab表单字段。\n",
        "\n",
        "'>'表示换行 ’enter‘表示空格\n",
        "\n",
        "> form supports multiple types of fields（字段）。 【 input fields； dropdown menus】 表单支持多种类型的字段； 【输入字段；下拉菜单】\n",
        "\n",
        "上个作业的E1 就是 你可以用double-clicking it \n",
        "\n",
        "changing valure in each input field (right) + check changes in the code(left) _vice versa\n",
        "代码对应文本\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6px568yg0-kM"
      },
      "source": [
        "#@title example form fields\n",
        "#@markdown please put description 请输入描述\n",
        "string = \"examples\" #@param {type:\"string\"}\n",
        "#表单汇总添加字段（右键最下面表单）\n",
        "# 只要加上花A功能块  后面的东西就可以显示出来。\n",
        "slider_value = 100 #@param{type:\"slider\", min:100, max:200}\n",
        "#slider_value 是个滚轴区间范围的滚轴；移动滚轴数值就在变化\n",
        "number =  1231231#@param{type:\"number\"}\n",
        " #数字而且（在警号花param之后不能在有注释了）\n",
        "data = '2020-01-05'#@param{type:\"date\"}\n",
        "pick_me = \"monday\" #@param['monday','tuesday','wednesday','thursday']\n",
        "select_or_input = \"\\u6709\\u70B9\\u50CF\\u8F93\\u5165\\u5355\\u8BCD\\uFF0C\\u7136\\u540E\\u5F39\\u51FA\\u4EC0\\u4E48\\u6765\" #@param[\"apples\",\"bananas\",\"oranges\"]{allow-input:true}\n",
        "#选择和输入\n",
        "\n",
        "'''\n",
        "param (泼·若啊·母)\n",
        "Embedded Image 英[ˌpærəˈm] Embedded Image 美[ˌpærəˈm] \n",
        "\n",
        "abbr. 参数（Parametric）\n",
        "date 直接弹出日期选择表单了\n",
        "#['1','2','3'……]下拉菜单 【\"\"，\"\"，\"\"】也是下拉菜单\n",
        "'''\n",
        "\n",
        "#print the output\n",
        "print(\"string is\",string)\n",
        "print('slider_value',slider_value)\n",
        "# 打印的话就是将名字直接放进print就行不需要再加''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHPD1ypLyPJZ"
      },
      "source": [
        "#string is examples\n",
        "#slider_value 111"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHfqy1zdEG3t"
      },
      "source": [
        "**测试TEST**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FKSco-Tpt4Be"
      },
      "source": [
        "print(filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8u346yLt2CH"
      },
      "source": [
        "filename"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEGnTW0QnnkO"
      },
      "source": [
        "result = wv_cbow_model.wv.most_similar(positive = ['woman','king'],negative =['man'],topn=10)\n",
        "result\n",
        "#和老师的lab结果还不太一样"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbLBVFXSdcLy"
      },
      "source": [
        "print(ft_sg_model) #FastText(vocab=21613, size=100, alpha=0.025)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6L6tEc-c0cA"
      },
      "source": [
        "ft_sg_model  #<gensim.models.fasttext.FastText at 0x7f23bdeacf50>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8of3Gs6wHyrP"
      },
      "source": [
        "print(wv_cbow_model) #Word2Vec(vocab=21613, size=100, alpha=0.025)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYiM-PlnHobO"
      },
      "source": [
        "wv_cbow_model #<gensim.models.word2vec.Word2Vec at 0x7f0a2c87e690>\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "48m5q6EIGEbw"
      },
      "source": [
        "sentences[:6]# 5句话，然后单词化了"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRg32fxjEJab"
      },
      "source": [
        "normalized_text[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDipJmJXbF_h"
      },
      "source": [
        "content_text[:1000] #个数；是字母的个数（获取文本）"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PmgMODZ-rFx"
      },
      "source": [
        "sent_text[:10] #十句话。"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auazkaj9-vZy"
      },
      "source": [
        "老师的\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2wnySpkzw2l",
        "outputId": "08caae38-1e94-4f44-9e7a-b66711d6a475"
      },
      "source": [
        "import pprint\n",
        "import re\n",
        "\n",
        "# For parsing our XML data\n",
        "from lxml import etree \n",
        "\n",
        "# For data processing\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# For implementing the word2vec family of algorithms\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDZnNVt3zzAd"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCq42Xr0zz2O"
      },
      "source": [
        "id = '1B47OiEiG2Lo1jUY6hy_zMmHBxfKQuJ8-'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('ted_en-20160408.xml')  "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pRG_R_Lz1YV",
        "outputId": "0d22a061-9608-43b8-b81d-a384d53fe5c7"
      },
      "source": [
        "targetXML=open('ted_en-20160408.xml', 'r', encoding='UTF8')\n",
        "\n",
        "# Getting contents of <content> tag from the xml file\n",
        "target_text = etree.parse(targetXML)\n",
        "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n",
        "\n",
        "# Removing \"Sound-effect labels\" using regular expression (regex) (i.e. (Audio), (Laughter))\n",
        "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
        "\n",
        "# Tokenising the sentence to process it by using NLTK library\n",
        "sent_text=sent_tokenize(content_text)\n",
        "\n",
        "# Removing punctuation and changing all characters to lower case\n",
        "normalized_text = []\n",
        "for string in sent_text:\n",
        "     tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
        "     normalized_text.append(tokens)\n",
        "\n",
        "# Tokenising each sentence to process individual word\n",
        "sentences=[]\n",
        "sentences=[word_tokenize(sentence) for sentence in normalized_text]\n",
        "\n",
        "# Prints only 10 (tokenised) sentences\n",
        "print(sentences[:10])\n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new'], ['to', 'me', 'the', 'real', 'real', 'solution', 'to', 'quality', 'growth', 'is', 'figuring', 'out', 'the', 'balance', 'between', 'two', 'activities', 'exploration', 'and', 'exploitation'], ['both', 'are', 'necessary', 'but', 'it', 'can', 'be', 'too', 'much', 'of', 'a', 'good', 'thing'], ['consider', 'facit'], ['i', 'm', 'actually', 'old', 'enough', 'to', 'remember', 'them'], ['facit', 'was', 'a', 'fantastic', 'company'], ['they', 'were', 'born', 'deep', 'in', 'the', 'swedish', 'forest', 'and', 'they', 'made', 'the', 'best', 'mechanical', 'calculators', 'in', 'the', 'world'], ['everybody', 'used', 'them'], ['and', 'what', 'did', 'facit', 'do', 'when', 'the', 'electronic', 'calculator', 'came', 'along'], ['they', 'continued', 'doing', 'exactly', 'the', 'same']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqQNDyQOz3kj"
      },
      "source": [
        "# Initialize and train a word2vec model with the following parameters:\n",
        "# sentence: iterable of iterables, i.e. the list of lists of tokens from our data\n",
        "# size: dimensionality of the word vectors\n",
        "# window: window size\n",
        "# min_count: ignores all words with total frequency lower than the specified count value\n",
        "# workers: Use specified number of worker threads to train the model (=faster training with multicore machines)\n",
        "# sg: training algorithm, 0 for CBOW, 1 for skip-gram\n",
        "wv_cbow_model = Word2Vec(sentences=sentences, size=100, window=5, min_count=5, workers=2, sg=0)\n",
        "#可能是这个地方出问题了。回去看看\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6UZ37Ymz3uG"
      },
      "source": [
        "# The trained word vectors are stored in a KeyedVectors instance as model.wv\n",
        "# Get the top 10 similar words to 'man' by calling most_similar() \n",
        "# most_similar() computes cosine similarity between a simple mean of the vectors of the given words and the vectors for each word in the model \n",
        "\n",
        "similar_words=wv_cbow_model.wv.most_similar(\"man\") # topn=10 by default\n",
        "pprint.pprint(similar_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qyc3-IXbz6Yl"
      },
      "source": [
        "# Now we switch to a Skip Gram model by setting parameter sg=1\n",
        "wv_sg_model = Word2Vec(sentences=sentences, size=100, window=5, min_count=5, workers=2, sg=1)\n",
        "#不是上面是这出了问题 不是这个出了问题 与空格无关系"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuUakUhoz6gw"
      },
      "source": [
        "similar_words=wv_sg_model.wv.most_similar(\"man\")\n",
        "pprint.pprint(similar_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTcM0kMJz6pu"
      },
      "source": [
        "from gensim.models import FastText"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vB0Y0CBz6x3"
      },
      "source": [
        "# Now we initialize and train FastText with Skip Gram architecture (sg=1)\n",
        "ft_sg_model = FastText(sentences, size=100, window=5, min_count=5, workers=2, sg=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5uyS6d50AWL"
      },
      "source": [
        "# As we can see, FastText allows us to obtain word vectors for out-of-vocabulary words\n",
        "result=ft_sg_model.wv.most_similar(\"electrofishing\")\n",
        "pprint.pprint(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ab822AO00A46"
      },
      "source": [
        "# Now we initialize and train FastText with CBOW architecture (sg=0)\n",
        "ft_cbow_model = FastText(sentences, size=100, window=5, min_count=5, workers=2, sg=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vglj1w6o0BCH"
      },
      "source": [
        "# Again, FastText allows us to obtain word vectors for out-of-vocabulary words\n",
        "result=ft_cbow_model.wv.most_similar(\"electrofishing\")\n",
        "pprint.pprint(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IvnkDZX0FgP"
      },
      "source": [
        "# We can specify the positive/negative word list with the positive/negative parameters\n",
        "# Top N most similar words can be specified with the topn parameter\n",
        "result = wv_cbow_model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
        "print(result)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84qW3nqM0FqA",
        "outputId": "86ad69c4-b8fb-480a-de19-ed8ca4e355d4"
      },
      "source": [
        "result = wv_sg_model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
        "print(result)\n",
        "#queen"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('luther', 0.6583076119422913)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1HDROre0F1E"
      },
      "source": [
        "result = ft_cbow_model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
        "print(result)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dcgna9dB0LMK"
      },
      "source": [
        "result = ft_sg_model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
        "print(result)\n",
        "#hawking"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRx5g7U00LXE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_PPUQuu0LjK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}