{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab2_learnbyself.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOWvOmMR011y+PE92Bzblz/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alecia113/NLP-Lab/blob/main/lab2_learnbyself.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRIcer6wvILs"
      },
      "source": [
        "# **Word2Vec**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8UuxxU0P88r",
        "outputId": "87ee10c4-6c77-4d0d-ca8f-6e131c68c7c6"
      },
      "source": [
        "#开始import\n",
        "import pprint\n",
        "import re\n",
        "\n",
        "from lxml import etree  # for parsing Our XML data  用于解析我们的XML数据。\n",
        "'''\n",
        "数据存储数据分析\n",
        "可扩展标记语言（标准通用标记语言的子集）是一种简单的数据存储语言。\n",
        "XML文件一般指里面写有可扩展标记语言的文件。 XML:可扩展标记语言,标准通用标记语言的子集,是一种用于标记电子文件使其具有结构性的标记语言。它被设计用来传输和存储数据。 XML,是Extensible Markup Language 的缩写。在.NET框架中XML是非常重要的一部分...”\n",
        "'''\n",
        "import nltk  # data precessing\n",
        "nltk.download('punkt')  #很固定\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "from gensim.models import Word2Vec  # implementing the word2vec family of algorithms.\n",
        "#用于实现word2vec 系列算法 ms。\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action = 'ignore', category = FutureWarning) "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "kih-fDj2Rszh",
        "outputId": "9dcb2943-f110-4aa1-d0df-5940feaa0d49"
      },
      "source": [
        "#https://www.cnblogs.com/leomei91/p/7660928.html\n",
        "#https://blog.csdn.net/weixin_42214743/article/details/107720074\n",
        "#有些警告是可以忽略的\n",
        "'''\n",
        "调用模型还是调整参数\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "具体函数：\n",
        "warnings.filterwarnings(action, message='', category=Warning, module='', lineno=0, append=False)\n",
        "'''"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n调用模型还是调整参数\\n\\nimport warnings\\nwarnings.filterwarnings('ignore')\\n\\n具体函数：\\nwarnings.filterwarnings(action, message='', category=Warning, module='', lineno=0, append=False)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aznd9pPMSHb1"
      },
      "source": [
        "**然后开始下载数据** use TED script(脚本） from google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjUFH3MfSQsW"
      },
      "source": [
        "# 下面代码是： generate a link +  a field (字段) ---> verification code (enter)\n",
        "#   click link  go to google sign in ; ; (own google account) \n",
        "#  copy  verification code + into field + press enter\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth  #GoogleAuthenticator(谷歌身份验证器)\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials  #谷歌  全权证书\n",
        "\n",
        "#Authenticate  [ɔːˈθentɪkeɪt]  v  鉴定，验证（凹三忒忒咳）\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9esb_dDkYP0U"
      },
      "source": [
        "下载TED Scripts from GD"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCxqrtAGY7wq"
      },
      "source": [
        "#点击files tab + see file is download successfully\n",
        "id = '1B47OiEiG2Lo1jUY6hy_zMmHBxfKQuJ8-'\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('ted_en-20160408.xml')\n",
        "\n",
        "#在路径中就会过个/content/ted_en-20160408.xml"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqybtF6HY3R_"
      },
      "source": [
        "**数据处理 data preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2U1sBHRiaGWR",
        "outputId": "81252a33-e533-4fb3-d032-38d7d2bec643"
      },
      "source": [
        "targetXML =open('ted_en-20160408.xml','r',encoding = 'UTF8') #等号后面必须要直接跟open；用utff8来打开ted文件\n",
        "#open(file, mode='r', buffering=-1, encoding=None, errors=None, newline=None, closefd=True, opener=None)\n",
        "\n",
        "target_text = etree.parse(targetXML)#etree.parse直接接受一个文档，按照文档结构解析（本地文件）\n",
        "'''\n",
        "etree.parse(*args, **kwargs)\n",
        "parse(source, parser=None, base_url=None)\n",
        "'''\n",
        "#etree.html可以解析html文件：（服务器上返回的html数据）\n",
        "#https://blog.csdn.net/cbiexi/article/details/104479744?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522161568688316780255255960%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=161568688316780255255960&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-3-104479744.first_rank_v2_pc_rank_v29&utm_term=etree.parse\n",
        "#https://blog.csdn.net/myself029/article/details/79954301?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522161568688316780255255960%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=161568688316780255255960&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-2-79954301.first_rank_v2_pc_rank_v29&utm_term=etree.parse\n",
        "#etree 更像是展示出来这个文本文档等等\n",
        "parse_text = '\\n'.join(target_text.xpath('//content/text()')) #获取<内容>标记的内容从xml 文件中【par解析文字】\n",
        "# 从xml文件中获取<content>标签的内容。\n",
        "# 【在爬取内容】\"\\n\".join() 来对字符串列表进行处理，不会出现不连贯情况\n",
        "#https://blog.csdn.net/freeking101/article/details/64461574?utm_medium=distribute.pc_relevant.none-task-blog-OPENSEARCH-1.control&dist_request_id=&depth_1-utm_source=distribute.pc_relevant.none-task-blog-OPENSEARCH-1.control\n",
        "#https://blog.csdn.net/weixin_39751871/article/details/111537728?ops_request_misc=&request_id=&biz_id=102&utm_term=%27%5Cn%27.join(target_text.xpath(%27/&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-0-111537728.first_rank_v2_pc_rank_v29\n",
        "\n",
        "\n",
        "content_text = re.sub(r'\\([^)]*\\)','',parse_text) # 删除音效标签（音频，笑声）#获得文本\n",
        "'''\n",
        "\\: 将下一个字符————》特殊字符、原义字符。[eg]:\\( ==(\n",
        "^ : 匹配输入字符串的开始位置\n",
        "*: 匹配前面的子表达式0次或者多次 * == {0,}\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "#regex 检索词、、正则表达式\n",
        "#正则表达式强大的文本匹配功能。很多文本匹配处理，如果没有正则表达式，还真的很难做出来。\n",
        "#https://blog.csdn.net/wyb880501/article/details/79813658\n",
        "\n",
        "sent_text = sent_tokenize(content_text) #利用NLTK库 对句子进行标记化处理。# 将句子分开。sent_tokenize\n",
        "#https://blog.csdn.net/wyb880501/article/details/79813658\n",
        "#regex(蕊·寨·课·嘶)n\n",
        "#https://blog.csdn.net/Wisimer/article/details/89556404?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522161569572316780271589227%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=161569572316780271589227&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-1-89556404.first_rank_v2_pc_rank_v29&utm_term=NLTK\n",
        "#NLTK 的讲解  \n",
        "\n",
        "#remove punctuation + change all characters --> lower case  去掉标点符号并将所有字符改为小写。\n",
        "normalized_text = []     #归一化文本  normalized (孬·某·赖·子·的)\n",
        " #从你的语句中获得单词\n",
        "for string in sent_text: \n",
        "  tokens = re.sub(r\"[^a-z0-9]+\",\" \",string.lower()) #[^a-z]负值字符范围;匹配任何不在指定范围内的任意字符;;匹配任何不在 ‘a' 到 ‘z' 范围内的任意字符。 标点符号。\n",
        "  normalized_text.append(tokens) # 把你处理好的东西丢到列表里。  #这个地方还是句子\n",
        "#tokenize （偷·可·耐·滋）\n",
        "# Tokenising (偷·可·耐·滋鹰）符号化 each sentence __process individual word #对每个句子进行符号化处理，以处理单个单词\n",
        "sentences=[]\n",
        "sentences=[word_tokenize(sentence) for sentence in normalized_text] #从文本冲取出来然后拆词\n",
        "\n",
        "#prints only 10 tokenised sentences  \n",
        "print(sentences[:10])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new'], ['to', 'me', 'the', 'real', 'real', 'solution', 'to', 'quality', 'growth', 'is', 'figuring', 'out', 'the', 'balance', 'between', 'two', 'activities', 'exploration', 'and', 'exploitation'], ['both', 'are', 'necessary', 'but', 'it', 'can', 'be', 'too', 'much', 'of', 'a', 'good', 'thing'], ['consider', 'facit'], ['i', 'm', 'actually', 'old', 'enough', 'to', 'remember', 'them'], ['facit', 'was', 'a', 'fantastic', 'company'], ['they', 'were', 'born', 'deep', 'in', 'the', 'swedish', 'forest', 'and', 'they', 'made', 'the', 'best', 'mechanical', 'calculators', 'in', 'the', 'world'], ['everybody', 'used', 'them'], ['and', 'what', 'did', 'facit', 'do', 'when', 'the', 'electronic', 'calculator', 'came', 'along'], ['they', 'continued', 'doing', 'exactly', 'the', 'same']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQQJOG8EGnEW"
      },
      "source": [
        "# **CBOW ——continuous bag of Words** Word2Vec 连续单词袋"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wjq5JioaG1lN"
      },
      "source": [
        "有关gensim.models.word2vec的更多详细信息，您可以参考Gensim Word2Vec的API。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "dCxH6Mb9GwYt",
        "outputId": "7edd0705-9a52-472a-d3f4-2550407fbb7f"
      },
      "source": [
        "#有关gensim.models.word2vec的更多详细信息，您可以参考Gensim Word2Vec的API。\n",
        "#https://radimrehurek.com/gensim/models/word2vec.html\n",
        "\n",
        "wv_cbow_model = Word2Vec(sentences=sentences, size=100, window=5, min_count=5 , workers=2, sg=0)\n",
        "\n",
        "'''\n",
        "Word2Vec(sentences=None, corpus_file=None, size=100, alpha=0.025, window=5, \n",
        "min_count=5, max_vocab_size=None, sample=0.001, seed=1, workers=3, \n",
        "min_alpha=0.0001, sg=0, hs=0, negative=5, ns_exponent=0.75, cbow_mean=1, \n",
        "hashfxn=<built-in function hash>, iter=5, null_word=0, trim_rule=None, \n",
        "sorted_vocab=1, batch_words=10000, compute_loss=False, callbacks=(), max_final_vocab=None)\n",
        "'''\n",
        "'''\n",
        " Initialize and train a word2vec model with the following parameters:\n",
        " 初始化+训练w2v模型\n",
        "# sentence: iterable of iterables, i.e. the list of lists of tokens from our data\n",
        "可迭代的迭代： 符号化来自我们数据的列表们的列表\n",
        "# size: dimensionality of the word vectors\n",
        "单词向量的维度\n",
        "# window: window size\n",
        "# min_count: ignores all words with total frequency lower than the specified count value\n",
        "频率比具体化数值低的单词都忽略。 （忽略总频率低语指定计数值的所有单词）\n",
        "\n",
        "# workers: Use specified number of worker threads （线程；主题）【嘶·若案·子】 to train the model (=faster training with multicore machines)\n",
        "\n",
        "运行程序就是进程 进程中最小单位是一个线程； 一个进程有至少一个线程\n",
        "清理垃圾之后再杀毒 是有顺序的，~~单线程 \n",
        "#https://blog.csdn.net/csdnnews/article/details/82321777?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522161569868516780264066483%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=161569868516780264066483&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-3-82321777.first_rank_v2_pc_rank_v29&utm_term=线程\n",
        "还讲了线程安全\n",
        "\n",
        "【worker是什么含义每天搞懂】使用指定数量的worker 线程来训练模型（=使用multicore机器进行更快的训练）\n",
        "\n",
        "# sg: training algorithm, 0 for CBOW, 1 for skip-gram\n",
        "sg：训练算法，CBOW 为0 ； Skip-gram 为1\n",
        "'''\n",
        "#这个地方空格了后面的地方没空格"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n Initialize and train a word2vec model with the following parameters:\\n 初始化+训练w2v模型\\n# sentence: iterable of iterables, i.e. the list of lists of tokens from our data\\n可迭代的迭代： 符号化来自我们数据的列表们的列表\\n# size: dimensionality of the word vectors\\n单词向量的维度\\n# window: window size\\n# min_count: ignores all words with total frequency lower than the specified count value\\n频率比具体化数值低的单词都忽略。 （忽略总频率低语指定计数值的所有单词）\\n\\n# workers: Use specified number of worker threads （线程；主题）【嘶·若案·子】 to train the model (=faster training with multicore machines)\\n\\n运行程序就是进程 进程中最小单位是一个线程； 一个进程有至少一个线程\\n清理垃圾之后再杀毒 是有顺序的，~~单线程 \\n#https://blog.csdn.net/csdnnews/article/details/82321777?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522161569868516780264066483%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=161569868516780264066483&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-3-82321777.first_rank_v2_pc_rank_v29&utm_term=线程\\n还讲了线程安全\\n\\n【worker是什么含义每天搞懂】使用指定数量的worker 线程来训练模型（=使用multicore机器进行更快的训练）\\n\\n# sg: training algorithm, 0 for CBOW, 1 for skip-gram\\nsg：训练算法，CBOW 为0 ； Skip-gram 为1\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OeZClxBQPEqm",
        "outputId": "982c8545-ca21-412d-98b9-63b189068abe"
      },
      "source": [
        "similar_words = wv_cbow_model.wv.most_similar(\"man\") #topn = 10 default (的A·佛奥·忒) 默认值\n",
        "pprint.pprint(similar_words)\n",
        "# The trained word vectors are stored in a KeyedVectors instance（实例） as model.wv\n",
        "#训练单词向量存在KeyedVectors 实例作为model.wv ([主宾谓]训练后的词向量以model.wv的形式存在keyedvec实例中)\n",
        "# Get the top 10 similar words to 'man' by calling most_similar() \n",
        "#通过calling most_similar() 得到 前十个相似单词。\n",
        "#通过调用most_similar() 获取与\"man\"相似的前十个词\n",
        "# most_similar() computes cosine similarity between a simple mean of the vectors of the given words and the vectors for each word in the model \n",
        "#他是变得。\n",
        "#most_similar() 计算余弦相似度，在一个简单的被给单词的向量含义和每个在模型中向量的单词\n",
        "#（计算给定的单词的向量和模型中每个单词的向量的简单均值之间的（ a simple mean of） 余弦相似度。\n",
        "#【不懂】\n",
        "#【值是变化的可能是获取的文件不够稳定。所有的值都是变化的。】但是频率差距不大。"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('woman', 0.8315678834915161),\n",
            " ('guy', 0.8118534088134766),\n",
            " ('lady', 0.7750102281570435),\n",
            " ('gentleman', 0.733288586139679),\n",
            " ('boy', 0.7258301973342896),\n",
            " ('girl', 0.7208313941955566),\n",
            " ('soldier', 0.715594470500946),\n",
            " ('poet', 0.6831284761428833),\n",
            " ('kid', 0.6823714971542358),\n",
            " ('friend', 0.6469538807868958)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "YGmwTT4ywGHI",
        "outputId": "b7eb00cb-b27c-45f2-aa2f-c7011712a73d"
      },
      "source": [
        "'''\n",
        "#sg=0；；cbow \n",
        "wv_cbow_model = Word2Vec(sentences = sentences, size = 100, window = 5, min_count = 5 , workers = 2, sg = 0)\n",
        "similar_words = wv_cbow_model.wv.most_similar(\"man\") \n",
        "\n",
        "#sg=1；； sg 模型\n",
        "wv_sg_model = Word2Vec(sentences=sentences, size=100,window=5,min_count=5,workers=2,sg=1)\n",
        "similar_words=wv_sg_model.wv.most_similar(\"man\") \n",
        "\n",
        "ft_sg_model = FastText(sentences,size=100,window=5,min_count=5,workers=2,sg=1)\n",
        "result=ft_sg_model.wv.most_similar(\"electrofishing\")\n",
        "\n",
        "ft_cbow_model = FastText(sentences,size=100,window=5,min_count=5,workers=2,sg=0)\n",
        "result=ft_cbow_model.wv.most_similar(\"electrofishing\")\n",
        "\n",
        "\n",
        "#就是把（）中的单词变成了K-M+W （）是变化的其他是不变的\n",
        "'''\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n#sg=0；；cbow \\nwv_cbow_model = Word2Vec(sentences = sentences, size = 100, window = 5, min_count = 5 , workers = 2, sg = 0)\\nsimilar_words = wv_cbow_model.wv.most_similar(\"man\") \\n\\n#sg=1；； sg 模型\\nwv_sg_model = Word2Vec(sentences=sentences, size=100,window=5,min_count=5,workers=2,sg=1)\\nsimilar_words=wv_sg_model.wv.most_similar(\"man\") \\n\\nft_sg_model = FastText(sentences,size=100,window=5,min_count=5,workers=2,sg=1)\\nresult=ft_sg_model.wv.most_similar(\"electrofishing\")\\n\\nft_cbow_model = FastText(sentences,size=100,window=5,min_count=5,workers=2,sg=0)\\nresult=ft_cbow_model.wv.most_similar(\"electrofishing\")\\n\\n\\n#就是把（）中的单词变成了K-M+W （）是变化的其他是不变的\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8-uvvr9Tk2Z"
      },
      "source": [
        "# Word2Vec-skip Gram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwFL6nQWTuwM"
      },
      "source": [
        "wv_sg_model = Word2Vec(sentences=sentences, size=100, window=5, min_count=5, workers=2, sg=1)\n",
        "# KG 和CBOW不同的是sg这个参数多了 从0变1 这样就从CBOW 模型切换到SG模型了。\n",
        " #跟空格有关系吗？？"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yIcEhosXEQ8",
        "outputId": "e55368c2-8470-4d17-c0e1-3f8ba5ccc2aa"
      },
      "source": [
        "similar_words=wv_sg_model.wv.most_similar(\"man\") #唯一不同把调用的模型名字变了。\n",
        "pprint.pprint(similar_words)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('woman', 0.7864292860031128),\n",
            " ('guy', 0.7424989938735962),\n",
            " ('soldier', 0.7129544019699097),\n",
            " ('rabbi', 0.703356146812439),\n",
            " ('boy', 0.7032188773155212),\n",
            " ('gentleman', 0.6906863451004028),\n",
            " ('pope', 0.677947461605072),\n",
            " ('imam', 0.6745090484619141),\n",
            " ('waitress', 0.6734718084335327),\n",
            " ('lady', 0.6605736613273621)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlK01yCVaC3b"
      },
      "source": [
        "# Word2Vec vs FastText"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZOxRJmWaIsC"
      },
      "source": [
        "Word2Vec-Skip Gram 不能发现相似文字去’electrofishing‘ 电子钓鱼；因为电子钓鱼不再词汇表中。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "madbMz6qag_8",
        "outputId": "8ea7000b-4631-4b2b-b568-d6cd65451037"
      },
      "source": [
        "similar_words=wv_sg_model.wv.most_similar(\"electrofishing\")\n",
        "pprint.pprint(similar_words)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-a2f7ec57b31e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msimilar_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwv_sg_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"electrofishing\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msimilar_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"word 'electrofishing' not in vocabulary\""
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-egW_iAaau4G"
      },
      "source": [
        "# FastText-Skip Gram"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "016nlVETa9RY"
      },
      "source": [
        "from gensim.models import FastText  #gensim.models 遗传因子模型\n",
        "'''\n",
        "gensim是一款强大的自然语言处理工具，里面包括N多常见模型：\n",
        "\n",
        "基本的语料处理工具\n",
        "LSI\n",
        "LDA\n",
        "HDP\n",
        "DTM\n",
        "DIM\n",
        "TF-IDF\n",
        "word2vec、paragraph2vec\n",
        ".\n",
        "#gensim详解 https://blog.csdn.net/sinat_26917383/article/details/69803018?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522161572013816780262535820%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&request_id=161572013816780262535820&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_v2~rank_v29-6-69803018.first_rank_v2_pc_rank_v29&utm_term=gensim.models+用法\n",
        "【好慢啊……】\n",
        "\n",
        "\n",
        "model=gensim.models.Word2Vec(sentences,sg=1,size=100,window=5,min_count=2,negative=3,sample=0.001,hs=1,workers=4)\n",
        " \n",
        "#该步骤也可分解为以下三步（但没必要）：\n",
        "#model=gensim.model.Word2Vec() 建立一个空的模型对象\n",
        "#model.build_vocab(sentences) 遍历一次语料库建立词典\n",
        "#model.train(sentences) 第二次遍历语料库建立神经网络模型\n",
        " \n",
        "#sg=1是skip—gram算法，对低频词敏感，默认sg=0为CBOW算法\n",
        "#size是神经网络层数，值太大则会耗内存并使算法计算变慢，一般值取为100到200之间。\n",
        "#window是句子中当前词与目标词之间的最大距离，3表示在目标词前看3-b个词，后面看b个词（b在0-3之间随机）\n",
        "#min_count是对词进行过滤，频率小于min-count的单词则会被忽视，默认值为5。\n",
        "#negative和sample可根据训练结果进行微调，sample表示更高频率的词被随机下采样到所设置的阈值，默认值为1e-3,\n",
        "#negative: 如果>0,则会采用negativesamping，用于设置多少个noise words\n",
        "#hs=1表示层级softmax将会被使用，默认hs=0且negative不为0，则负采样将会被选择使用。\n",
        "#workers是线程数，此参数只有在安装了Cpython后才有效，否则只能使用单核\n",
        " \n",
        "# min_count，是去除小于min_count的单词\n",
        "# size，神经网络层数\n",
        "# sg， 算法选择\n",
        "# window， 句子中当前词与目标词之间的最大距离\n",
        "# workers，线程数\n",
        "#model.save(\"文本名\")\t#模型会保存到该 .py文件同级目录下，该模型打开为乱码\n",
        "\n",
        "\n",
        "和上面那个是一个包\n",
        "\n",
        "'''\n",
        "ft_sg_model = FastText(sentences,size=100,window=5,min_count=5,workers=2,sg=1)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "id": "UCQ9a6XdeCWr",
        "outputId": "dcdf8ffc-5d3c-436d-ef9c-a9e63024abc1"
      },
      "source": [
        "result=ft_sg_model.wv.most_similar(\"electrofishing\") #.wv.most_similar这个是固定的前面的模块不同\n",
        "pprint.pprint(result) #截断的\n",
        "'''\n",
        "skip gram 架构（sg=1) 初始化和训练fasttext\n",
        "fasttext允许我们获取词汇外单词的单词向量\n",
        "'''"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('electrolux', 0.804796040058136),\n",
            " ('electro', 0.8027043342590332),\n",
            " ('electrolyte', 0.7923442721366882),\n",
            " ('electroshock', 0.7875984907150269),\n",
            " ('electric', 0.7694752216339111),\n",
            " ('electronic', 0.7653239965438843),\n",
            " ('electron', 0.7637100219726562),\n",
            " ('electrogram', 0.7581183910369873),\n",
            " ('electrons', 0.7571943998336792),\n",
            " ('fishing', 0.7532349824905396)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nskip gram 架构（sg=1) 初始化和训练fasttext\\nfasttext允许我们获取词汇外单词的单词向量\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zE7kVcKZgu1Q"
      },
      "source": [
        "# FastText-CBOW (continuous bag-of-words)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeS-MWuug273"
      },
      "source": [
        "#initialize + train FastText with CBOW architecture(sg=0)\n",
        "#现在我们使用CBOW的构架初始化和训练FastText （sg=0)  \n",
        "ft_cbow_model = FastText(sentences,size=100,window=5,min_count=5,workers=2,sg=0)\n",
        "#[训练依旧很慢]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0NVH934iC3W",
        "outputId": "42531fb0-7407-46f8-c50c-fec9d4544844"
      },
      "source": [
        "result=ft_cbow_model.wv.most_similar(\"electrofishing\") #不训练的话；就会：AttributeError: 'FastText' object has no attribute 'wv_most_similar\n",
        "pprint.pprint(result)\n",
        "#这里是wv。most-"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('electric', 0.9142772555351257),\n",
            " ('electro', 0.9090090990066528),\n",
            " ('electrolux', 0.9042349457740784),\n",
            " ('electron', 0.8834762573242188),\n",
            " ('electroshock', 0.8756364583969116),\n",
            " ('electrolyte', 0.8753466010093689),\n",
            " ('electrode', 0.8724755644798279),\n",
            " ('electromagnet', 0.87140291929245),\n",
            " ('electronic', 0.863182783126831),\n",
            " ('electrical', 0.8630403280258179)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHjyESOTiU6t"
      },
      "source": [
        "# **King -Man +Woman = ? **"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXDHo7J8iyeX"
      },
      "source": [
        "**Try  both CBOW + SKIP Gram model compute \"k-M +W **"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzyLyJDTzhR0"
      },
      "source": [
        "得到的词的不一样呢"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RpdLeVRojEGD",
        "outputId": "0db7cb92-3424-4269-fee3-0f32d6b183cb"
      },
      "source": [
        "# sepcify positive/negative word list ~~~ positive/negative parameters\n",
        "#我们可以使用正/负参数指定正/负词列表\n",
        "# Top N most similar words can be specified with topn parameter\n",
        "#可以使用topn参数是定前N个最相似的单词。\n",
        "result = wv_cbow_model.wv.most_similar(positive = ['woman','king'],negative =['man'],topn=1)\n",
        "#正的就是positive 负的就是negative\n",
        "print(result) #指定前1个最相似的"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('president', 0.7821868658065796)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BF68elivjEPA",
        "outputId": "a1088420-c10a-4764-cf03-abfdfd0405dc"
      },
      "source": [
        "result = wv_sg_model.wv.most_similar(positive=['woman','king'],negative=['man'],topn=1)\n",
        "print(result) #wv_sg   #这个地方单词输入很大啊。也不知道怎么了；也不是这句话的问题\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('luther', 0.6786664128303528)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxuwJD3zpQIw",
        "outputId": "d7128dfb-0cb1-486d-cb69-a7c4b7682f6e"
      },
      "source": [
        "result = ft_cbow_model.wv.most_similar(positive=['woman','king'],negative=['man'],topn=1)\n",
        "print(result) #ft_cbow "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('kidding', 0.8905613422393799)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26rFqrKKp2Fm",
        "outputId": "c0218783-1db2-41ce-94ec-1780c867e6b4"
      },
      "source": [
        "result = ft_sg_model.wv.most_similar(positive=['woman','king'],negative=['man'],topn=1)\n",
        "print(result)  #ft_sg  sg"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('pauling', 0.6824973821640015)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JN5hTwnIsp5B"
      },
      "source": [
        "没有足够的数据来回答“女王”，下面我们将用更大的数据进行训练。（google 已经使用了google news 数据训练了Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmoHfP2xuPll"
      },
      "source": [
        "# Using Pretrained word embeddings with Gensim ；在Gensim中使用预训练词嵌入"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMPeDkkB1ttj"
      },
      "source": [
        "从google预训练的word2Vec二进制文件下载并加载"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nze_4vGI1aH_"
      },
      "source": [
        "#https://code.google.com/archive/p/word2vec/  #连接到项目\n",
        "# Download the pre-trained vectors trained on part of Google News dataset (about 100 billion words)\n",
        "# Beware, this file is big (3.39GB) - might be long waiting!\n",
        "# 1000亿个单词  下载在google 新闻数据集的一部分中 经过训练的预训练向量。（3.39GB）\n",
        "id2 = '0B7XkCwpI5KDYNlNUTTlSS21pQmM'\n",
        "downloaded = drive.CreateFile({'id':id2})\n",
        "downloaded.GetContentFile('GoogleNews-vectors-negative300.bin.gz') "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5-fAA8x34s8",
        "outputId": "270dd91e-9f95-4c92-e989-ca8e5be81ce7"
      },
      "source": [
        "#Uncompress the downloaded file 解压下载的文件。\n",
        "! gzip -d /content/GoogleNews-vectors-negative300.bin.gz\n",
        "#zip: /content/GoogleNews-vectors-negative300.bin already exists; do you wish to overwrite (y or n)?\n",
        "#zip。/content/GoogleNews-vectors-negative300.bin已经存在，你要不要覆盖（y或n）？"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gzip: /content/GoogleNews-vectors-negative300.bin already exists; do you wish to overwrite (y or n)? n\n",
            "\tnot overwritten\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dm9Jr0uI4Tpq"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "#load pretrained vectors with keyedvectors instance(example eg) : might be long waiting!\n",
        "filename = 'GoogleNews-vectors-negative300.bin'\n",
        "gn_mv_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)\n",
        "#以姿态将KeyedVectors 加载到预训练的向量中-可能要等很久\n",
        "#851-52"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yy8gJfiQtB8v",
        "outputId": "5ddc599e-5765-49ba-ccf1-0a011d73382d"
      },
      "source": [
        "# now we can try calculate \"King-man+woman = ?\" again\n",
        "result = gn_mv_model.most_similar(positive=['woman','king'], negative=['man'], topn=1)\n",
        "print(result)\n",
        "#还真是ram不够用中断了\n",
        "#老师[（'queen'，0.7118192911148071）]\n",
        "#这个结果就正确了"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('queen', 0.7118192911148071)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2g8bfU7ovbEN"
      },
      "source": [
        "# load a pretained word embedding model using API ; \n",
        "\n",
        "> 使用API加载预训练的词嵌入模型\n",
        "\n",
        "> another way: use Gensim to load pretrained word\n",
        "\n",
        "> try GloVe embedding trained on twitter data尝试对Twitter数据进行训练的GloVe嵌入\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHa052uTwFbq",
        "outputId": "7b1b6053-b76e-43f4-8d65-cc47eea71e64"
      },
      "source": [
        "import gensim.downloader as api\n",
        " \n",
        "model = api.load(\"glove-twitter-25\") #download model + return as object ready for use(下载模型备用；作为对象返回)\n",
        "print(model.similarity(\"cat\",\"dog\"))#similarity function can calculate the cosine similarity between 2 give words.\n",
        "#similarity() 可以计算两个给定单词的余弦相似度\n",
        "#99.3% 104.1/104.8MB downloaded0.95908207   0.040917932987213135 老师的\n",
        "#为什么没加载完呢？\n",
        "#0.95908207\n",
        "print(model.distance(\"cat\",\"dog\"))\n",
        "# a another way calculating the similarity between 2 given words ; returns 1-cosine  similarity instead \n",
        "'''\n",
        "0.95908207\n",
        "0.040917932987213135\n",
        "'''\n",
        "#就算是没加载完结果还是一样。而且会用到RAM将近10个G\n",
        "#第二个是返回1-cosine 相似度\n",
        "#余弦距离 = 1 - 余弦相似度，其取值范围为[ 0， 2 ]，即相同的两个向量余弦距离为0\n",
        "'''\n",
        "欧氏距离体现数值上的绝对差异，而余弦距离体现方向上的相对差异。\n",
        "余弦距离不是严格定义上的距离！\n",
        "https://blog.csdn.net/m0_38024592/article/details/100075613?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522161575999916780265455768%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=161575999916780265455768&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~baidu_landing_v2~default-1-100075613.first_rank_v2_pc_rank_v29&utm_term=1-cosine+\n",
        "\n",
        "\n",
        "'''\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.95908207\n",
            "0.040917932987213135\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yutG8CmTz4XQ"
      },
      "source": [
        "# 【tips】 paly with colab form fields 玩colab表单字段。\n",
        "\n",
        "'>'表示换行 ’enter‘表示空格\n",
        "\n",
        "> form supports multiple types of fields（字段）。 【 input fields； dropdown menus】 表单支持多种类型的字段； 【输入字段；下拉菜单】\n",
        "\n",
        "上个作业的E1 就是 你可以用double-clicking it \n",
        "\n",
        "changing valure in each input field (right) + check changes in the code(left) _vice versa\n",
        "代码对应文本\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6px568yg0-kM",
        "outputId": "7da9faba-a85b-48fe-ba23-c9275e49daa5"
      },
      "source": [
        "#@title example form fields\n",
        "#@markdown please put description 请输入描述\n",
        "string = \"examples\" #@param {type:\"string\"}\n",
        "#表单汇总添加字段（右键最下面表单）\n",
        "# 只要加上花A功能块  后面的东西就可以显示出来。\n",
        "slider_value = 100 #@param{type:\"slider\", min:100, max:200}\n",
        "#slider_value 是个滚轴区间范围的滚轴；移动滚轴数值就在变化\n",
        "number =  1231231#@param{type:\"number\"}\n",
        " #数字而且（在警号花param之后不能在有注释了）\n",
        "data = '2020-01-05'#@param{type:\"date\"}\n",
        "pick_me = \"monday\" #@param['monday','tuesday','wednesday','thursday']\n",
        "select_or_input = \"\\u6709\\u70B9\\u50CF\\u8F93\\u5165\\u5355\\u8BCD\\uFF0C\\u7136\\u540E\\u5F39\\u51FA\\u4EC0\\u4E48\\u6765\" #@param[\"apples\",\"bananas\",\"oranges\"]{allow-input:true}\n",
        "#选择和输入\n",
        "\n",
        "'''\n",
        "param (泼·若啊·母)\n",
        "Embedded Image 英[ˌpærəˈm] Embedded Image 美[ˌpærəˈm] \n",
        "\n",
        "abbr. 参数（Parametric）\n",
        "date 直接弹出日期选择表单了\n",
        "#['1','2','3'……]下拉菜单 【\"\"，\"\"，\"\"】也是下拉菜单\n",
        "'''\n",
        "\n",
        "#print the output\n",
        "print(\"string is\",string)\n",
        "print('slider_value',slider_value)\n",
        "# 打印的话就是将名字直接放进print就行不需要再加''"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "string is examples\n",
            "slider_value 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHPD1ypLyPJZ"
      },
      "source": [
        "#string is examples\n",
        "#slider_value 111"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHfqy1zdEG3t"
      },
      "source": [
        "**测试TEST**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKSco-Tpt4Be",
        "outputId": "fa34c06c-5903-4602-a8fe-222cdb6bfac7"
      },
      "source": [
        "print(filename)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GoogleNews-vectors-negative300.bin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "c8u346yLt2CH",
        "outputId": "ef822a01-fdcf-46fe-e941-5882ce47f778"
      },
      "source": [
        "filename"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'GoogleNews-vectors-negative300.bin'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEGnTW0QnnkO",
        "outputId": "c06ba621-b6e3-4ea3-a93f-c25c732f7501"
      },
      "source": [
        "result = wv_cbow_model.wv.most_similar(positive = ['woman','king'],negative =['man'],topn=10)\n",
        "result\n",
        "#和老师的lab结果还不太一样"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('president', 0.7821868658065796),\n",
              " ('luther', 0.742743194103241),\n",
              " ('charles', 0.7388191223144531),\n",
              " ('john', 0.7344681024551392),\n",
              " ('french', 0.72505122423172),\n",
              " ('dr', 0.7189538478851318),\n",
              " ('minister', 0.7162536382675171),\n",
              " ('nelson', 0.715813159942627),\n",
              " ('named', 0.7118213176727295),\n",
              " ('mary', 0.7100918889045715)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbLBVFXSdcLy",
        "outputId": "da992a7b-d4ea-487a-8cf1-3512cb8c547a"
      },
      "source": [
        "print(ft_sg_model) #FastText(vocab=21613, size=100, alpha=0.025)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FastText(vocab=21613, size=100, alpha=0.025)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6L6tEc-c0cA",
        "outputId": "1137af24-a46a-4593-ddc9-5599ded0fc0c"
      },
      "source": [
        "ft_sg_model  #<gensim.models.fasttext.FastText at 0x7f23bdeacf50>"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<gensim.models.fasttext.FastText at 0x7faeb0526190>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8of3Gs6wHyrP",
        "outputId": "f7be832c-c6ed-421b-f1cd-1f2b14570855"
      },
      "source": [
        "print(wv_cbow_model) #Word2Vec(vocab=21613, size=100, alpha=0.025)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word2Vec(vocab=21613, size=100, alpha=0.025)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYiM-PlnHobO",
        "outputId": "7bc02782-0b88-4f25-f884-86622a773ca0"
      },
      "source": [
        "wv_cbow_model #<gensim.models.word2vec.Word2Vec at 0x7f0a2c87e690>\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<gensim.models.word2vec.Word2Vec at 0x7faed38f8650>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48m5q6EIGEbw",
        "outputId": "d76f7f0f-37c3-453f-87e3-825a0fd4c96c"
      },
      "source": [
        "sentences[:6]# 5句话，然后单词化了"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['here',\n",
              "  'are',\n",
              "  'two',\n",
              "  'reasons',\n",
              "  'companies',\n",
              "  'fail',\n",
              "  'they',\n",
              "  'only',\n",
              "  'do',\n",
              "  'more',\n",
              "  'of',\n",
              "  'the',\n",
              "  'same',\n",
              "  'or',\n",
              "  'they',\n",
              "  'only',\n",
              "  'do',\n",
              "  'what',\n",
              "  's',\n",
              "  'new'],\n",
              " ['to',\n",
              "  'me',\n",
              "  'the',\n",
              "  'real',\n",
              "  'real',\n",
              "  'solution',\n",
              "  'to',\n",
              "  'quality',\n",
              "  'growth',\n",
              "  'is',\n",
              "  'figuring',\n",
              "  'out',\n",
              "  'the',\n",
              "  'balance',\n",
              "  'between',\n",
              "  'two',\n",
              "  'activities',\n",
              "  'exploration',\n",
              "  'and',\n",
              "  'exploitation'],\n",
              " ['both',\n",
              "  'are',\n",
              "  'necessary',\n",
              "  'but',\n",
              "  'it',\n",
              "  'can',\n",
              "  'be',\n",
              "  'too',\n",
              "  'much',\n",
              "  'of',\n",
              "  'a',\n",
              "  'good',\n",
              "  'thing'],\n",
              " ['consider', 'facit'],\n",
              " ['i', 'm', 'actually', 'old', 'enough', 'to', 'remember', 'them'],\n",
              " ['facit', 'was', 'a', 'fantastic', 'company']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRg32fxjEJab",
        "outputId": "fb850995-0706-4d83-9ba3-08772d49b310"
      },
      "source": [
        "normalized_text[:10]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['here are two reasons companies fail they only do more of the same or they only do what s new ',\n",
              " 'to me the real real solution to quality growth is figuring out the balance between two activities exploration and exploitation ',\n",
              " 'both are necessary but it can be too much of a good thing ',\n",
              " 'consider facit ',\n",
              " 'i m actually old enough to remember them ',\n",
              " 'facit was a fantastic company ',\n",
              " 'they were born deep in the swedish forest and they made the best mechanical calculators in the world ',\n",
              " 'everybody used them ',\n",
              " 'and what did facit do when the electronic calculator came along ',\n",
              " 'they continued doing exactly the same ']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 177
        },
        "id": "cDipJmJXbF_h",
        "outputId": "d7c3b9e3-e7c1-413d-f2eb-008a289f624e"
      },
      "source": [
        "content_text[:1000] #个数；是字母的个数（获取文本）"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Here are two reasons companies fail: they only do more of the same, or they only do what's new.\\nTo me the real, real solution to quality growth is figuring out the balance between two activities: exploration and exploitation. Both are necessary, but it can be too much of a good thing.\\nConsider Facit. I'm actually old enough to remember them. Facit was a fantastic company. They were born deep in the Swedish forest, and they made the best mechanical calculators in the world. Everybody used them. And what did Facit do when the electronic calculator came along? They continued doing exactly the same. In six months, they went from maximum revenue ... and they were gone. Gone.\\nTo me, the irony about the Facit story is hearing about the Facit engineers, who had bought cheap, small electronic calculators in Japan that they used to double-check their calculators.\\n\\nFacit did too much exploitation. But exploration can go wild, too.\\nA few years back, I worked closely alongside a European biotech co\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PmgMODZ-rFx",
        "outputId": "78201b08-c480-4420-9154-cfcb4d5f94a3"
      },
      "source": [
        "sent_text[:10] #十句话。"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Here are two reasons companies fail: they only do more of the same, or they only do what's new.\",\n",
              " 'To me the real, real solution to quality growth is figuring out the balance between two activities: exploration and exploitation.',\n",
              " 'Both are necessary, but it can be too much of a good thing.',\n",
              " 'Consider Facit.',\n",
              " \"I'm actually old enough to remember them.\",\n",
              " 'Facit was a fantastic company.',\n",
              " 'They were born deep in the Swedish forest, and they made the best mechanical calculators in the world.',\n",
              " 'Everybody used them.',\n",
              " 'And what did Facit do when the electronic calculator came along?',\n",
              " 'They continued doing exactly the same.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auazkaj9-vZy"
      },
      "source": [
        "老师的\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2wnySpkzw2l",
        "outputId": "d985f080-df19-4eef-ffe7-0708bb58b7b4"
      },
      "source": [
        "import pprint\n",
        "import re\n",
        "\n",
        "# For parsing our XML data\n",
        "from lxml import etree \n",
        "\n",
        "# For data processing\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# For implementing the word2vec family of algorithms\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDZnNVt3zzAd"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCq42Xr0zz2O"
      },
      "source": [
        "id = '1B47OiEiG2Lo1jUY6hy_zMmHBxfKQuJ8-'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('ted_en-20160408.xml')  "
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pRG_R_Lz1YV",
        "outputId": "7e066a38-7d2a-4c05-f88e-59e5e03aed10"
      },
      "source": [
        "targetXML=open('ted_en-20160408.xml', 'r', encoding='UTF8')\n",
        "\n",
        "# Getting contents of <content> tag from the xml file\n",
        "target_text = etree.parse(targetXML)\n",
        "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n",
        "\n",
        "# Removing \"Sound-effect labels\" using regular expression (regex) (i.e. (Audio), (Laughter))\n",
        "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
        "\n",
        "# Tokenising the sentence to process it by using NLTK library\n",
        "sent_text=sent_tokenize(content_text)\n",
        "\n",
        "# Removing punctuation and changing all characters to lower case\n",
        "normalized_text = []\n",
        "for string in sent_text:\n",
        "     tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
        "     normalized_text.append(tokens)\n",
        "\n",
        "# Tokenising each sentence to process individual word\n",
        "sentences=[]\n",
        "sentences=[word_tokenize(sentence) for sentence in normalized_text]\n",
        "\n",
        "# Prints only 10 (tokenised) sentences\n",
        "print(sentences[:10])"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['here', 'are', 'two', 'reasons', 'companies', 'fail', 'they', 'only', 'do', 'more', 'of', 'the', 'same', 'or', 'they', 'only', 'do', 'what', 's', 'new'], ['to', 'me', 'the', 'real', 'real', 'solution', 'to', 'quality', 'growth', 'is', 'figuring', 'out', 'the', 'balance', 'between', 'two', 'activities', 'exploration', 'and', 'exploitation'], ['both', 'are', 'necessary', 'but', 'it', 'can', 'be', 'too', 'much', 'of', 'a', 'good', 'thing'], ['consider', 'facit'], ['i', 'm', 'actually', 'old', 'enough', 'to', 'remember', 'them'], ['facit', 'was', 'a', 'fantastic', 'company'], ['they', 'were', 'born', 'deep', 'in', 'the', 'swedish', 'forest', 'and', 'they', 'made', 'the', 'best', 'mechanical', 'calculators', 'in', 'the', 'world'], ['everybody', 'used', 'them'], ['and', 'what', 'did', 'facit', 'do', 'when', 'the', 'electronic', 'calculator', 'came', 'along'], ['they', 'continued', 'doing', 'exactly', 'the', 'same']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqQNDyQOz3kj"
      },
      "source": [
        "# Initialize and train a word2vec model with the following parameters:\n",
        "# sentence: iterable of iterables, i.e. the list of lists of tokens from our data\n",
        "# size: dimensionality of the word vectors\n",
        "# window: window size\n",
        "# min_count: ignores all words with total frequency lower than the specified count value\n",
        "# workers: Use specified number of worker threads to train the model (=faster training with multicore machines)\n",
        "# sg: training algorithm, 0 for CBOW, 1 for skip-gram\n",
        "wv_cbow_model = Word2Vec(sentences=sentences, size=100, window=5, min_count=5, workers=2, sg=0)\n",
        "#可能是这个地方出问题了。回去看看\n"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6UZ37Ymz3uG",
        "outputId": "ddf9a38f-9244-4923-a062-7a429e94de45"
      },
      "source": [
        "# The trained word vectors are stored in a KeyedVectors instance as model.wv\n",
        "# Get the top 10 similar words to 'man' by calling most_similar() \n",
        "# most_similar() computes cosine similarity between a simple mean of the vectors of the given words and the vectors for each word in the model \n",
        "\n",
        "similar_words=wv_cbow_model.wv.most_similar(\"man\") # topn=10 by default\n",
        "pprint.pprint(similar_words)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('woman', 0.8713363409042358),\n",
            " ('guy', 0.8023020625114441),\n",
            " ('lady', 0.7739849090576172),\n",
            " ('girl', 0.7508987188339233),\n",
            " ('boy', 0.7496033906936646),\n",
            " ('gentleman', 0.7343142628669739),\n",
            " ('kid', 0.7237218618392944),\n",
            " ('soldier', 0.7182086706161499),\n",
            " ('surgeon', 0.6886041164398193),\n",
            " ('poet', 0.685889720916748)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qyc3-IXbz6Yl"
      },
      "source": [
        "# Now we switch to a Skip Gram model by setting parameter sg=1\n",
        "wv_sg_model = Word2Vec(sentences=sentences, size=100, window=5, min_count=5, workers=2, sg=1)\n",
        "#不是上面是这出了问题"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuUakUhoz6gw",
        "outputId": "8fe8f38c-115b-4299-a824-3da1546a4c93"
      },
      "source": [
        "similar_words=wv_sg_model.wv.most_similar(\"man\")\n",
        "pprint.pprint(similar_words)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('woman', 0.7973149418830872),\n",
            " ('guy', 0.720652163028717),\n",
            " ('soldier', 0.717073917388916),\n",
            " ('girl', 0.7015227675437927),\n",
            " ('boy', 0.6948396563529968),\n",
            " ('gentleman', 0.6824488043785095),\n",
            " ('lady', 0.6811760663986206),\n",
            " ('pianist', 0.6785898208618164),\n",
            " ('titus', 0.6740497350692749),\n",
            " ('michelangelo', 0.6716635227203369)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTcM0kMJz6pu"
      },
      "source": [
        "from gensim.models import FastText"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vB0Y0CBz6x3"
      },
      "source": [
        "# Now we initialize and train FastText with Skip Gram architecture (sg=1)\n",
        "ft_sg_model = FastText(sentences, size=100, window=5, min_count=5, workers=2, sg=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5uyS6d50AWL"
      },
      "source": [
        "# As we can see, FastText allows us to obtain word vectors for out-of-vocabulary words\n",
        "result=ft_sg_model.wv.most_similar(\"electrofishing\")\n",
        "pprint.pprint(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ab822AO00A46"
      },
      "source": [
        "# Now we initialize and train FastText with CBOW architecture (sg=0)\n",
        "ft_cbow_model = FastText(sentences, size=100, window=5, min_count=5, workers=2, sg=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vglj1w6o0BCH"
      },
      "source": [
        "# Again, FastText allows us to obtain word vectors for out-of-vocabulary words\n",
        "result=ft_cbow_model.wv.most_similar(\"electrofishing\")\n",
        "pprint.pprint(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IvnkDZX0FgP",
        "outputId": "e795d24d-a696-4328-81e9-0e71a742189c"
      },
      "source": [
        "# We can specify the positive/negative word list with the positive/negative parameters\n",
        "# Top N most similar words can be specified with the topn parameter\n",
        "result = wv_cbow_model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
        "print(result)\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('president', 0.8262898921966553)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84qW3nqM0FqA",
        "outputId": "2c2972c2-5235-4c13-8036-b2397dfc750b"
      },
      "source": [
        "result = wv_sg_model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
        "print(result)\n",
        "#queen"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('luther', 0.7065996527671814)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1HDROre0F1E",
        "outputId": "cac83dcb-c33b-4050-8455-e30dba3bc445"
      },
      "source": [
        "result = ft_cbow_model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
        "print(result)\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('kidding', 0.8905613422393799)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dcgna9dB0LMK",
        "outputId": "d4240d05-07ff-4fd6-fcdf-35e8057434d3"
      },
      "source": [
        "result = ft_sg_model.wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
        "print(result)\n",
        "#hawking"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('pauling', 0.6824973821640015)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRx5g7U00LXE"
      },
      "source": [
        ""
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_PPUQuu0LjK"
      },
      "source": [
        ""
      ],
      "execution_count": 39,
      "outputs": []
    }
  ]
}