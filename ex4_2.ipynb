{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ex4.2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOA4hbRUmQiCRNGHWZqkf56",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alecia113/NLP-Lab/blob/main/ex4_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poV2vRI49fSu"
      },
      "source": [
        "双向RNN\n",
        "单项RNN 是从前往后把单词复制到序列里面，这样的话对于第i个序列的单词，它只有前i-1个序列的信息，如果我们再加上一个从后往前的序列，第i个单词就同时包含了，0到i-1，以及从i+1 到最后的单词的信息，这样的话对于整个模型来说，就可以更好的理解单词上下文的意思，对于单词表达也会更加清晰一点。\n",
        "\n",
        "双向RNN增加了后面一些单词的信息，因为单词的上下文都很重要。\n",
        "比如，the name of your son\n",
        "如果你从前往后得到the name 这个东西；它是不知道name是有什么含义的\n",
        "因为name是指你儿子的名字，必须要把后面的意思也理解清楚，这样才能更好的理解the name"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9S8r1fCi65YA"
      },
      "source": [
        "# Code to download file into Colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "# Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "id = '16bT0gkTI3EEemyi-6rMaZ-bJHfsCbFGU'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('embedded_docs.pkl')  \n",
        "\n",
        "id = '1xjgvacGXn5hSWERBw5zTAFtHessESCb-'\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('labels.pkl')  \n",
        "\n",
        "import pickle\n",
        "input_embeddings = pickle.load(open(\"embedded_docs.pkl\",\"rb\"))\n",
        "label = pickle.load(open(\"labels.pkl\",\"rb\"))"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjbF_RSo7EUc",
        "outputId": "71d066c3-ca68-4b98-f109-01392a2afc2b"
      },
      "source": [
        "# Split into training and testing dataset using scikit-learn\n",
        "# For more details, you can refer to: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_embeddings, test_embeddings, train_label, test_label = train_test_split(input_embeddings,label,test_size = 0.2, random_state=0)\n",
        "\n",
        "# The processed data has the shape of (data_size, sequence_length, embedding_dimension)\n",
        "print(train_embeddings.shape)\n",
        "print(train_label.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1805, 512, 25)\n",
            "(1805,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQre93yC7HaO"
      },
      "source": [
        "import torch\n",
        "# You can enable GPU here (cuda); or just CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "2RZlnfd-7LYV",
        "outputId": "b95618e6-b34f-44d0-bfd6-adf472134975"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Bi_RNN_Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Bi_RNN_Model, self).__init__()\n",
        "        # Please complete the code for the modeling part\n",
        "        self.rnn = nn.RNN(n_input, n_hidden, num_layers = 2, batch_first = True, dropout = 0.2, bidirectional = True)\n",
        "        self.linear = nn.Linear(n_hidden*2, n_class)\n",
        "\n",
        "    def forward(self, x):        \n",
        "        # Please complete the code for forward propagation\n",
        "        rnn_output, h_n = self.rnn(x)\n",
        "        x_last = torch.cat((h_n[2,:], h_n[3,:]), 1)\n",
        "        output = self.linear(x_last)\n",
        "        return output\n",
        "\n",
        "'''\n",
        " C=torch.cat((A,B),0)#按维数0（行）拼接\n",
        "  C=torch.cat((A,D),1)#按维数1（列）拼接\n",
        "'''\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n C=torch.cat((A,B),0)#按维数0（行）拼接\\n  C=torch.cat((A,D),1)#按维数1（列）拼接\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UolPhiv58KbT"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2o7Bjt6R8ZFe"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cfyuCZV7Lw6"
      },
      "source": [
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "\n",
        "# Please assign values to these variables by using other variables (instead of hard code)\n",
        "# This should be the length of each sequence\n",
        "seq_length = 512\n",
        "# This should be the input feature dimension\n",
        "n_input = 25\n",
        "# This should be the number of class\n",
        "n_class = 4\n",
        "\n",
        "#Please decide the hyperparameters here by yourself\n",
        "n_hidden = 64\n",
        "batch_size = 128\n",
        "total_epoch = 20\n",
        "learning_rate = 0.001"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MKPW4JZy7NRV",
        "outputId": "481538ac-8d3e-40d3-a01f-ca11cefe876e"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "model = Bi_RNN_Model().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Please find which optimizer provides a higher f1\n",
        "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
        "\n",
        "for epoch in range(total_epoch):\n",
        "    train_loss = 0\n",
        "    for ind in range(0,train_embeddings.shape[0],batch_size):\n",
        "        input_batch = train_embeddings[ind:min(ind+batch_size, train_embeddings.shape[0])]\n",
        "        target_batch = train_label[ind:min(ind+batch_size, train_embeddings.shape[0])]\n",
        "        input_batch_torch = torch.from_numpy(input_batch).float().to(device)\n",
        "        target_batch_torch = torch.from_numpy(target_batch).view(-1).to(device)\n",
        "\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_batch_torch) \n",
        "        loss = criterion(outputs, target_batch_torch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        train_loss += loss.item()\n",
        "\n",
        "    print('Epoch: %d, train loss: %.5f'%(epoch + 1, train_loss))\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "## Prediction\n",
        "model.eval()\n",
        "outputs = model(torch.from_numpy(test_embeddings).float().to(device)) \n",
        "predicted = torch.argmax(outputs, 1)\n",
        "\n",
        "# classification_report builds a text report showing the main classification metrics\n",
        "# The returned report includes the 'weighted avg f1' we want (refer to the sample output)\n",
        "# More details can be found from: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(test_label, predicted.cpu().numpy(),digits=4))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, train loss: 20.33082\n",
            "Epoch: 2, train loss: 19.56509\n",
            "Epoch: 3, train loss: 18.93629\n",
            "Epoch: 4, train loss: 18.17504\n",
            "Epoch: 5, train loss: 17.26345\n",
            "Epoch: 6, train loss: 16.57210\n",
            "Epoch: 7, train loss: 15.77502\n",
            "Epoch: 8, train loss: 15.13489\n",
            "Epoch: 9, train loss: 14.64007\n",
            "Epoch: 10, train loss: 14.01177\n",
            "Epoch: 11, train loss: 13.42441\n",
            "Epoch: 12, train loss: 12.93753\n",
            "Epoch: 13, train loss: 12.16001\n",
            "Epoch: 14, train loss: 10.78142\n",
            "Epoch: 15, train loss: 10.20065\n",
            "Epoch: 16, train loss: 9.95631\n",
            "Epoch: 17, train loss: 9.85323\n",
            "Epoch: 18, train loss: 9.45550\n",
            "Epoch: 19, train loss: 8.49514\n",
            "Epoch: 20, train loss: 7.86050\n",
            "Finished Training\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7396    0.7634    0.7513        93\n",
            "           1     0.7381    0.7623    0.7500       122\n",
            "           2     0.6571    0.6330    0.6449       109\n",
            "           3     0.7440    0.7266    0.7352       128\n",
            "\n",
            "    accuracy                         0.7212       452\n",
            "   macro avg     0.7197    0.7213    0.7203       452\n",
            "weighted avg     0.7206    0.7212    0.7207       452\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uuHWgRJa7RGH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}