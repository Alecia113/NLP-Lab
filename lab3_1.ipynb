{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab3_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPry+MHYdtSTUZBO9Y/jcpM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alecia113/NLP-Lab/blob/main/lab3_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWQeQDbaGUnC"
      },
      "source": [
        "# **Neural Network Example**实战了有点\n",
        "#identifying animal species (features)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IE8KNEpMGmJt"
      },
      "source": [
        "import torch\n",
        "import numpy as np"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZF7WPOwGq7T"
      },
      "source": [
        "two features: two aspects \n",
        "1: animal have hair\n",
        "2: animal have feathers\n",
        "has hair [011000]\n",
        "has feather [001001]\n",
        "----input ^\n",
        "0-Oher 1-Manmmal 2-Bird [012002]\n",
        "---output/y ^"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgKImD0lHmu2"
      },
      "source": [
        "# aim: predict animal == bird/ manmmal/others"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThpMFoWwHx-l"
      },
      "source": [
        "'''\n",
        "创建数据one-hot\n",
        "Create our feature data\n",
        "[hair,feather]:0: X; 1:✅\n",
        "[1,0]: animal has hair + doesn't have feathers\n",
        "'''\n",
        "\n",
        "'''\n",
        "转换数据；变张量\n",
        "transform data --> torch data type (Pytorch)\n",
        "【传说中的如何让老师进行测试】\n",
        "# Uncomment the following line to print out if you want to see the details\n",
        "# print(x_data_torch)\n",
        "'''\n",
        "\n",
        "'''\n",
        "好像是将输出的数据也录入了数组。\n",
        "0-other； 1-mammal 2-bird\n",
        "#然后再转换成张量\n",
        "'''\n",
        "\n",
        "'''\n",
        "记录数据特征，和输出类的数量;;;就是赋值\n",
        "'''\n",
        "#这个部分更像是准备数据\n",
        "#print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLCQB_crIObM"
      },
      "source": [
        "#perpare for data\n",
        "x_data = np.array(\n",
        "    [[0,0],[1,0],[1,1],[0,0],[0,0],[0,1]])  #创建数据one-hot\n",
        "\n",
        "x_data_torch = torch.from_numpy(x_data).float() #转类型变张量，目前还是整数\n",
        "\n",
        "y_data = np.array([0,1,2,0,0,2]) #output\n",
        "y_data_torch = torch.from_numpy(y_data) #output torch\n",
        "\n",
        "num_features = 2 # 2个特征--hair, feather\n",
        "num_classes = 3 # 3 输出类 other,mammal,bird"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiTa6k1yNCPq"
      },
      "source": [
        "**No hidden layer --- manual 🤚 parameter update (weight + bias)** EG\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2Z_fdTrTAGV"
      },
      "source": [
        "'''\n",
        "调包\n",
        "nn\n",
        "和矩阵准确率分数\n",
        "'''\n",
        "'''\n",
        "initialize weight + bias manually🤚 + setup gredient\n",
        "torch.randn returns --->tensor (random fill with) (from normal distribution with mean 0 + vaiance方差 1)\n",
        "[standard normal distribution 标准正态分布]\n",
        "  # 功能：从标准正态分布（均值为0，方差为1）中抽取的一组随机数。返回一个张量\n",
        "sizes (int…) - 整数序列，定义输出张量的形状\n",
        "out (Tensor, optinal) - 结果张量\n",
        "\n",
        "该方法能够决定自动梯度机制是否需要为当前这个张量计算记录运算操作.\n",
        "该方法能对当前张量的requires_grad属性进行原地操作.返回这个当前张量.\n",
        "\n",
        "'''\n",
        "# Learning Rate - determines the step size at each iteration while moving toward a minimum of a loss function\n",
        "\n",
        "# Epoch - A measure of the number of times all of the training vectors are used once to update the weights.\n",
        "#方法训练向量的次数。（训练向量来每次更新权重）\n",
        "#尝试2000看会发生什么\n",
        "\n",
        "'''\n",
        "for 循环开始正向传播 forward propagation   calculation+ storage of intermediate variables(incl. outputs)\n",
        "from input layer --> output layer \n",
        "\n",
        "'''\n",
        "\n",
        "'''\n",
        "softmax change each value--->(0,1) + all values  +  add up(sum) = 1\n",
        "eg:  [8.04, 2.76, -6.52] -> [0.53 0.24 0.23]\n",
        "(l唠割锐泽母 log_softmax )\n",
        "\n",
        "#啊哈！算法；\n",
        "#算法图解\n",
        "先进行softmax 然后进行logarithm softmax 对数\n",
        "softmax： exp(x_i) / exp(x).sum() and log_softmax: log(exp(x_i)) /exp(x).sum() ) #只是分子做log\n",
        "log(softmax(x)) 但是实际做起来 is different + efficient\n",
        "    # https://pytorch.org/docs/stable/generated/torch.nn.LogSoftmax.html#torch.nn.LogSoftmax\n",
        "    def log_softmax(input: Tensor, dim: Optional[int]=None, _stacklevel: \n",
        "      int=3, dtype: Optional[int]=None) ->Tensor\n",
        "'''\n",
        "\n",
        "#然后损失 开始要对比情况了。计算消极log 可能损失;[计算负对数似然损失]\n",
        "'''\n",
        "torch.nn.functional.nll_loss(input, target, weight=None, \n",
        "size_average=None, ignore_index=-100, reduce=None, reduction='mean')\n",
        "\n",
        "一般只赋值前两个张量就可以了\n",
        "'''\n",
        "\n",
        "'''\n",
        "开始反向传播：back propagation 进行计算梯度。 优化WB 使用学习率 -学习率*W/B 的梯度数据  然后再清零\n",
        "torch.no_grad()\n",
        "https://blog.csdn.net/weixin_46559271/article/details/105658654?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522161629843016780357253890%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=161629843016780357253890&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-1-105658654.first_rank_v2_pc_rank_v29&utm_term=torch.no_grad\n",
        "当你做反向传播时，不要累积梯度。\n",
        "'''\n",
        "\n",
        "'''\n",
        "\n",
        "argmax函数：torch.argmax(input, dim=None, keepdim=False)返回指定维度最大值的序号，dim给定的定义是：the demention to reduce.也就是把dim这个维度的，变成这个维度的最大值的index。\n",
        "\n",
        "https://blog.csdn.net/weixin_42494287/article/details/92797061?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522161630304416780269870820%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&request_id=161630304416780269870820&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_v2~rank_v29-1-92797061.first_rank_v2_pc_rank_v29&utm_term=torch.argmax做什么用的\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayovWGA6Nbau",
        "outputId": "01f7f8df-1f82-40e2-d186-bc764460a236"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "W1 = torch.randn(num_features, num_classes, requires_grad=True)#torch.randn(2,3)\n",
        "B1 = torch.randn(num_classes, requires_grad=True) #3\n",
        "\n",
        "learning_rate = 0.01 \n",
        "\n",
        "number_of_epochs = 2000\n",
        "\n",
        "for epoch in range(number_of_epochs):\n",
        "\n",
        "  z =torch.add(torch.matmul(x_data_torch, W1),B1) # 输入两个单词张量，和weight 和bias、 forward propagation；【其实感觉还是在整合数据】\n",
        "  \n",
        "  log_softmax = F.log_softmax(z,dim=1)  #nn.functional.log_softmax   【softmax--log】\n",
        "\n",
        "  loss = F.nll_loss(log_softmax, y_data_torch)  # input target  ----loss【一个对数概率向量和一个目标标签】\n",
        "\n",
        "  loss.backward() #calculate gradient\n",
        "  with torch.no_grad():   #进入with 调用torch.no_grad 返回一个grad对象 torch.no_grad() 是一个上下文管理器，被该语句 wrap 起来的部分将不会track 梯度。\n",
        "      W1.data -= learning_rate*W1.grad.data   #gradient descent\n",
        "      B1.data -= learning_rate*B1.grad.data\n",
        "  W1.grad.data.zero_() #reset gradient\n",
        "  B1.grad.data.zero_()\n",
        "\n",
        "  if epoch % 200 == 199:\n",
        "    with torch.no_grad():        #预测不需要梯度的\n",
        "        pred_outputs = torch.add(torch.matmul(x_data_torch ,W1),B1)  #重新开始做乘法，回到了最开始的时候。\n",
        "        predicted = torch.argmax(pred_outputs, 1)  #不是.outputs 是——\n",
        "        train_acc = accuracy_score(predicted.numpy(), y_data)\n",
        "        print('Epoch:%d, loss: %.4f, train_acc:%.3f' %(epoch + 1, loss.item() , train_acc))\n",
        "\n",
        "#result\n",
        "print('Predicted :', predicted.numpy())\n",
        "print('Truth :', y_data)\n",
        "print('Accuracy : %.2f' %train_acc)\n",
        "\n",
        "\n",
        "#训练准确度在增加，损失在降低\n",
        "'''\n",
        "\n",
        "Epoch:200, loss: 1.8851, train_acc:0.167\n",
        "Epoch:400, loss: 1.1088, train_acc:0.833\n",
        "Epoch:600, loss: 0.7822, train_acc:0.833\n",
        "Epoch:800, loss: 0.6057, train_acc:1.000\n",
        "Epoch:1000, loss: 0.4985, train_acc:1.000\n",
        "Epoch:1200, loss: 0.4256, train_acc:1.000\n",
        "Epoch:1400, loss: 0.3718, train_acc:1.000\n",
        "Epoch:1600, loss: 0.3302, train_acc:1.000\n",
        "Epoch:1800, loss: 0.2968, train_acc:1.000\n",
        "Epoch:2000, loss: 0.2694, train_acc:1.000\n",
        "Predicted : [0 1 2 0 0 2]\n",
        "Truth : [0 1 2 0 0 2]\n",
        "Accuracy : 1.00[suji]\n",
        "\n",
        "有hidden\n",
        "\n",
        "Epoch: 200, loss: 0.9641, train_acc: 0.500\n",
        "Epoch: 400, loss: 0.8275, train_acc: 0.667\n",
        "Epoch: 600, loss: 0.7101, train_acc: 0.833\n",
        "Epoch: 800, loss: 0.6079, train_acc: 0.833\n",
        "Epoch: 1000, loss: 0.5234, train_acc: 0.833\n",
        "Epoch: 1200, loss: 0.4527, train_acc: 0.833\n",
        "Epoch: 1400, loss: 0.3903, train_acc: 0.833\n",
        "Epoch: 1600, loss: 0.3342, train_acc: 1.000\n",
        "Epoch: 1800, loss: 0.2883, train_acc: 1.000\n",
        "Epoch: 2000, loss: 0.2466, train_acc: 1.000\n",
        "Finished\n",
        "Predicted ： [0 1 2 0 0 2]\n",
        "Truth : [0 1 2 0 0 2]\n",
        "Accuracy : 1.00[suiji]\n",
        "\n",
        "'''"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:200, loss: 1.8851, train_acc:0.167\n",
            "Epoch:400, loss: 1.1088, train_acc:0.833\n",
            "Epoch:600, loss: 0.7822, train_acc:0.833\n",
            "Epoch:800, loss: 0.6057, train_acc:1.000\n",
            "Epoch:1000, loss: 0.4985, train_acc:1.000\n",
            "Epoch:1200, loss: 0.4256, train_acc:1.000\n",
            "Epoch:1400, loss: 0.3718, train_acc:1.000\n",
            "Epoch:1600, loss: 0.3302, train_acc:1.000\n",
            "Epoch:1800, loss: 0.2968, train_acc:1.000\n",
            "Epoch:2000, loss: 0.2694, train_acc:1.000\n",
            "Predicted : [0 1 2 0 0 2]\n",
            "Truth : [0 1 2 0 0 2]\n",
            "Accuracy : 1.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLj1ptKARpdk"
      },
      "source": [
        "# 换了顺序： Hidden layer + manual parameter update  有H + 🤚"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzE3AFkzkw2b"
      },
      "source": [
        "'''\n",
        "in hidden layer neurons numbers\n",
        "定义隐藏层的神经元 \n",
        "\n",
        "input features, output neurons(in hidden layer)(numbers)--W1\n",
        "\n",
        "B1=== let Bias(hidden layer) output candidates numbers.  (neurons in hidden layer 5)\n",
        "\n",
        "wout input （neurons numbers [hidden], output(classes))\n",
        "\n",
        "'''\n",
        "#定义神经元构建WB 从单词到隐藏层神经元。   b 让b输出日志数量（神经元）【这地方不理解】 \n",
        "#w out 从中间到边上，输出大类。   Bout = 设置bias 为输出候选数。即3（类数）\n",
        "#设置学习率 和 周期\n",
        "'''\n",
        "然后开始进行内部的。从添加。\n",
        "F.relu 激活函数。然后进行激活函数\n",
        "然后进行softmax log 然后 import torch.nn.functional as F\n",
        "fit函数中采用损失函数F.nll_loss()\n",
        "NLLLoss 的 输入 是一个对数概率向量和一个目标标签. 它不会为我们计算对数概率. 适合网络的最后一层是log_softmax\n",
        "----^还在准备数据\n",
        "'''\n",
        "#激活函数准备输出的做分析的数据；；；对数softmax\n",
        "#损失loss，   反向传播损失\n",
        "#反向传播损失\n",
        "#然后学习率训练数据，（调学习率） 每次处理完都要清除数据zero"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yT9335p2RVLk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e7699d2-9d6b-49af-d4b4-dc0119d59e5c"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "n_hidden_1 = 5 #隐藏层中的神经元\n",
        "\n",
        "W1 = torch.randn(num_features, n_hidden_1, requires_grad=True) #输入特征，输出（隐藏侧神经元数量）\n",
        "B1 = torch.randn(n_hidden_1, requires_grad=True) # 让隐藏层；偏移输出 候选日期的数量。即5 隐藏层中的神经元数量\n",
        "# w1 （ ……num_features， num_classes， requ）\n",
        "#Bout = 没隐藏的b\n",
        "Wout = torch.rand(n_hidden_1, num_classes, requires_grad=True)#输入神经元数，输出类\n",
        "\n",
        "Bout = torch.randn(num_classes, requires_grad=True)  #输出类3\n",
        "\n",
        "learning_rate=0.01 #没变\n",
        "no_of_epochs = 2000 # nob\n",
        "\n",
        "for epoch in range(no_of_epochs):\n",
        "  z1 = torch.add(torch.matmul(x_data_torch, W1), B1) # no 变\n",
        "  Zout = torch.add(torch.matmul(F.relu(z1), Wout), Bout) #  激活函数的加和\n",
        "\n",
        "  log_softmax = F.log_softmax(Zout,dim=1) #no大变\n",
        "  loss = F.nll_loss(log_softmax, y_data_torch) #no大变\n",
        "\n",
        "  loss.backward()#nb\n",
        "  with torch.no_grad():#nb\n",
        "    W1.data -= learning_rate*W1.grad.data#nb\n",
        "    B1.data -= learning_rate*B1.grad.data#nb\n",
        "    \n",
        "    Wout.data -= learning_rate*Wout.grad.data  #bian\n",
        "    Bout.data -= learning_rate*Bout.grad.data  #b\n",
        "\n",
        "  W1.grad.data.zero_()#nb\n",
        "  B1.grad.data.zero_()#nb\n",
        "  Wout.grad.data.zero_()#b\n",
        "  Bout.grad.data.zero_()#b\n",
        "\n",
        "\n",
        "  if epoch % 200 == 199:#nd\n",
        "    with torch.no_grad():#nd\n",
        "      z1 = torch.add(torch.matmul(x_data_torch ,W1),B1) #nd\n",
        "      Zout = torch.add(torch.matmul(F.relu(z1),Wout),Bout)#d\n",
        "      predicted = torch.argmax(Zout,1)#lued\n",
        "      train_acc = accuracy_score(predicted.numpy(),y_data)#nd\n",
        "      print('Epoch: %d, loss: %.4f, train_acc: %.3f' %(epoch +1, loss.item() , train_acc))\n",
        "#nd\n",
        "print(\"Finished\") #分隔\n",
        "\n",
        "#result\n",
        "print('Predicted ：', predicted.numpy()) #Predicted ： [0 1 2 0 0 2]\n",
        "print('Truth :', y_data)#Trure ： [0 1 2 0 0 2]\n",
        "print('Accuracy : %.2f' %train_acc) #Accuracy :\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 200, loss: 0.9641, train_acc: 0.500\n",
            "Epoch: 400, loss: 0.8275, train_acc: 0.667\n",
            "Epoch: 600, loss: 0.7101, train_acc: 0.833\n",
            "Epoch: 800, loss: 0.6079, train_acc: 0.833\n",
            "Epoch: 1000, loss: 0.5234, train_acc: 0.833\n",
            "Epoch: 1200, loss: 0.4527, train_acc: 0.833\n",
            "Epoch: 1400, loss: 0.3903, train_acc: 0.833\n",
            "Epoch: 1600, loss: 0.3342, train_acc: 1.000\n",
            "Epoch: 1800, loss: 0.2883, train_acc: 1.000\n",
            "Epoch: 2000, loss: 0.2466, train_acc: 1.000\n",
            "Finished\n",
            "Predicted ： [0 1 2 0 0 2]\n",
            "Truth : [0 1 2 0 0 2]\n",
            "Accuracy : 1.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d_-QPWaZRVpX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94ZOPrNbHzxw"
      },
      "source": [
        "#**TEST**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-87AU7wE6Y2A",
        "outputId": "331108a3-44ed-4286-a5f9-11acc0c352e4"
      },
      "source": [
        "#2.4\n",
        "Bout.data -= learning_rate*Bout.grad.data  #b\n",
        "print(Bout.data)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([-1.9189, -0.1212,  0.2067])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m643ySif6Oh9",
        "outputId": "d9241ac0-34cc-4cda-978e-0e3e8ba5e76b"
      },
      "source": [
        "#2.3\n",
        "Zout = torch.add(torch.matmul(F.relu(z1), Wout), Bout) \n",
        "print(Zout)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.5934,  0.7461,  1.3760],\n",
            "        [-1.4566, -0.0133,  0.5646],\n",
            "        [-1.8992,  0.0396,  0.2620],\n",
            "        [-0.5934,  0.7461,  1.3760],\n",
            "        [-0.5934,  0.7461,  1.3760],\n",
            "        [-1.8286,  0.6143,  0.4600]], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DM8vDadV6EJP",
        "outputId": "41aca658-f19f-49a3-9378-091e6f03e3b2"
      },
      "source": [
        "#2.2\n",
        "Wout = torch.rand(n_hidden_1, num_classes, requires_grad=True)\n",
        "print(Wout)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.7730, 0.1804, 0.5983],\n",
            "        [0.9212, 0.4871, 0.7432],\n",
            "        [0.5285, 0.3293, 0.5707],\n",
            "        [0.7976, 0.9384, 0.7502],\n",
            "        [0.0467, 0.3825, 0.1316]], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CLvbRdoVR2ic",
        "outputId": "7b8706ef-5f92-4c79-d34e-4febcc024f3c"
      },
      "source": [
        "#2.1\n",
        "Zout = torch.add(torch.matmul(F.relu(z1),Wout),Bout)\n",
        "print(Zout)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 3.5850, -0.1930,  0.2251],\n",
            "        [-0.6030,  0.0824, -0.4356],\n",
            "        [-1.3784, -0.2961,  0.7000],\n",
            "        [ 3.5850, -0.1930,  0.2251],\n",
            "        [ 3.5850, -0.1930,  0.2251],\n",
            "        [ 0.5547, -0.9210,  2.4629]], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fW8AlLHGRX0s"
      },
      "source": [
        "没隐藏层手动调参。 测试如下。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bj6P5yPpMnJy",
        "outputId": "08bd5ca2-aeea-46bc-978c-cb625cc9fef6"
      },
      "source": [
        "#16\n",
        "tain_acc = accuracy_score(predicted.numpy(), y_data)\n",
        "print(tain_acc)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXupZX1qMUbI",
        "outputId": "0c2b5ebc-6bbe-4a31-c05f-d55b0db134d8"
      },
      "source": [
        "#15\n",
        "predicted = torch.argmax(pred_outputs, 1)\n",
        "print(predicted)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0, 1, 2, 0, 0, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "945ciGNxMFu3",
        "outputId": "15c647f8-c707-4f0c-9f73-98945a94beda"
      },
      "source": [
        "#14\n",
        "print(pred_outputs)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 1.3947,  0.6585,  0.5634],\n",
            "        [ 0.9237, -0.4688,  0.6831],\n",
            "        [-1.6573,  1.1094,  1.3526],\n",
            "        [ 1.3947,  0.6585,  0.5634],\n",
            "        [ 1.3947,  0.6585,  0.5634],\n",
            "        [-1.1862,  2.2367,  1.2329]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qBVVQhb8JTZ1",
        "outputId": "5697a777-5d1d-4091-a040-24fa8adca807"
      },
      "source": [
        "#13\n",
        "B1.data -= learning_rate*B1.grad.data\n",
        "print(B1.data)\n",
        "#tensor([ 2.1613,  0.7510, -0.7514],  #tensor([ 34.8541,  -0.3064, -33.4468])"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 34.8541,  -0.3064, -33.4468])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4w55lxdsH-YB",
        "outputId": "8bf88958-d56d-44cb-8633-f057cbf35b2b"
      },
      "source": [
        "#12\n",
        "W1.data -= learning_rate*W1.grad.data\n",
        "print(W1.data)\n",
        "\n",
        "\n",
        "'''\n",
        "tensor([[ 0.9477,  1.5061, -0.7059],\n",
        "        [-0.3978, -0.5679, -0.0613]\n",
        "        tensor([[ 0.3480,  0.0354,  0.5222],\n",
        "        [ 2.0451, -0.0493,  1.2841]])\n",
        "        '''"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.3480,  0.0354,  0.5222],\n",
            "        [ 2.0451, -0.0493,  1.2841]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eKzzYiTH551",
        "outputId": "8bd9aae9-2c6f-4e2f-d15c-20da46054136"
      },
      "source": [
        "#11\n",
        "loss = F.nll_loss(log_softmax, y_data_torch)\n",
        "print(loss)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1.6704, grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q6H5td2MERT_",
        "outputId": "0ffb0e72-e23b-4517-bf25-573798fbc8bb"
      },
      "source": [
        "#10\n",
        "log_softmax = F.log_softmax(z,dim=1)\n",
        "print(log_softmax)\n",
        "\n",
        "#https://blog.csdn.net/hao5335156/article/details/80607732?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522161629884116780265451148%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=161629884116780265451148&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-80607732.first_rank_v2_pc_rank_v29&utm_term=F.log_softmax\n",
        "#加快计算速度，数值上也更稳定。\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-0.2611, -1.6715, -3.1738],\n",
            "        [-0.2168, -1.9441, -2.9598],\n",
            "        [-0.0504, -3.8751, -3.5608],\n",
            "        [-0.2611, -1.6715, -3.1738],\n",
            "        [-0.2611, -1.6715, -3.1738],\n",
            "        [-0.0537, -3.5615, -3.7339]], grad_fn=<LogSoftmaxBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRoUblpt41wH",
        "outputId": "bede2b65-930b-45cc-a09a-4bd97302a8ec"
      },
      "source": [
        "#9 torch.matmul 可以输入高维的；二维就是图通乘法\n",
        "#输入多维就是把拖出来的一位作为batch 提出来。其他部分做矩阵乘法\n",
        "#将b的第0维 5 broadcast 提出来。后两维做矩阵乘法\n",
        "import pprint\n",
        "a = torch.ones(3,4)\n",
        "b = torch.ones(5,4,2)\n",
        "c = torch.matmul(a,b)\n",
        "print(c.shape)\n",
        "pprint.pprint(c)\n",
        "#https://blog.csdn.net/qsmx666/article/details/105783610?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522161629814016780271553753%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&request_id=161629814016780271553753&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_v2~rank_v29-4-105783610.first_rank_v2_pc_rank_v29&utm_term=torch.matmul参数怎么用\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5, 3, 2])\n",
            "tensor([[[4., 4.],\n",
            "         [4., 4.],\n",
            "         [4., 4.]],\n",
            "\n",
            "        [[4., 4.],\n",
            "         [4., 4.],\n",
            "         [4., 4.]],\n",
            "\n",
            "        [[4., 4.],\n",
            "         [4., 4.],\n",
            "         [4., 4.]],\n",
            "\n",
            "        [[4., 4.],\n",
            "         [4., 4.],\n",
            "         [4., 4.]],\n",
            "\n",
            "        [[4., 4.],\n",
            "         [4., 4.],\n",
            "         [4., 4.]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xb4iwTMW29ED",
        "outputId": "993fee4a-6517-4744-fdc5-62e5f42e4c33"
      },
      "source": [
        "#8\n",
        "z =torch.add(torch.matmul(x_data_torch, W1),B1) #参数传递到 torch.add 后返回输入参数的求和结果作为输出\n",
        "#torch.matmul 是常用乘法\n",
        "print(z)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 2.1613,  0.7510, -0.7514],\n",
            "        [ 2.5123,  0.7850, -0.2307],\n",
            "        [ 4.5605,  0.7358,  1.0501],\n",
            "        [ 2.1613,  0.7510, -0.7514],\n",
            "        [ 2.1613,  0.7510, -0.7514],\n",
            "        [ 4.2096,  0.7018,  0.5294]], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EKSDj8g24hf",
        "outputId": "a4c30f12-dc20-4234-9740-eafce6d6e97d"
      },
      "source": [
        "#7\n",
        "B1 = torch.randn(num_classes, requires_grad=True)\n",
        "print(B1)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 2.1613,  0.7510, -0.7514], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgLWHdZ-UGfW",
        "outputId": "5a04070d-0f6e-4b3d-dadc-9cab6c5f0e1e"
      },
      "source": [
        "W1 = torch.randn(num_features, num_classes, requires_grad=True)\n",
        "print(W1)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.9477,  1.5061, -0.7059],\n",
            "        [-0.3978, -0.5679, -0.0613]], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95YTLYnWMyEJ",
        "outputId": "ed3a7bd3-a67d-4e71-e4e9-2e68eba7ec11"
      },
      "source": [
        "#5\n",
        "num_features = 2 # 2个特征--hair, feather\n",
        "num_classes = 3 # 3 输出类 other,mammal,bird\n",
        "print(num_features,num_classes)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0Rmcm2-LlBV",
        "outputId": "43508915-3370-4140-a8ef-612b2bd933b4"
      },
      "source": [
        "#4\n",
        "y_data_torch = torch.from_numpy(y_data)\n",
        "print(y_data_torch)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0, 1, 2, 0, 0, 2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYI10BEFJ-jz",
        "outputId": "db621d25-ba54-4b4d-c745-c24096695b65"
      },
      "source": [
        "#3\n",
        "y_data = np.array([0,1,2,0,0,2])\n",
        "print(y_data)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 2 0 0 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3P7lRrFIJLRA",
        "outputId": "88489beb-0a19-4602-b079-23846c4b9ad6"
      },
      "source": [
        "#2\n",
        "x_data_torch = torch.from_numpy(x_data)\n",
        "print(x_data_torch)\n",
        "x_data_torch = torch.from_numpy(x_data).float()\n",
        "print(x_data_torch)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0, 0],\n",
            "        [1, 0],\n",
            "        [1, 1],\n",
            "        [0, 0],\n",
            "        [0, 0],\n",
            "        [0, 1]])\n",
            "tensor([[0., 0.],\n",
            "        [1., 0.],\n",
            "        [1., 1.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09hJb5nSH2di",
        "outputId": "e3fc01fc-1558-40de-92b2-59f3e90cd481"
      },
      "source": [
        "#1\n",
        "x_data = np.array(\n",
        "    [[0,0],[1,0],[1,1],[0,0],[0,0],[0,1]])\n",
        "print(x_data)#竖着全打出来了 也就是一行为【】然后整个【】再（）放在数组当中"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 0]\n",
            " [1 0]\n",
            " [1 1]\n",
            " [0 0]\n",
            " [0 0]\n",
            " [0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}