{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab4.1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPPNQg0KiEAAxMHAkjJ6O9s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alecia113/NLP-Lab/blob/main/lab4_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-Os8_WS3mTJ"
      },
      "source": [
        "Recurrent Neural Networks + Sequence Modelling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAuGdg_j4eJM"
      },
      "source": [
        "#recurrent （复发的 蕊·科·run3·忒） sequence （计数，序数） （C 困·嘶） neural（牛肉）\n",
        "#RNN ； seq2seq\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# you can GPU(cuda)/ just CPU"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxOSsoZA5WdZ"
      },
      "source": [
        "#RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "afnsAoF96a3g",
        "outputId": "3ab3fd3e-f237-4542-8656-2a2b938a4c34"
      },
      "source": [
        "'''\n",
        "RNN 是一类神经网络。其中节点之间的连接形成沿着时间序列的有向图。 \n",
        "时间手段跨时间分布。\n",
        "将它与分布在空间中的网络进行比较。这可让他表现出时间动态行为。\n",
        "RNN 源自前馈神经网络。\n",
        "可以使用其内部状态（内存）来处理可变长度的输入序列\n",
        "\n",
        "'''"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nRNN 是一类神经网络。其中节点之间的连接形成沿着时间序列的有向图。 \\n时间手段跨时间分布。\\n将它与分布在空间中的网络进行比较。这可让他表现出时间动态行为。\\nRNN 源自前馈神经网络。\\n可以使用其内部状态（内存）来处理可变长度的输入序列\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4LzEjeO7D8W"
      },
      "source": [
        "#**predict the last character of the word** 预测单词的最后一个字符"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b4h-xy_7Gkp"
      },
      "source": [
        "**eg：\"wor\"-->\"d\", \"dee\"-->\"p\"**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OFcMG5RCtfa"
      },
      "source": [
        "##有什么用呢？？？？？？"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qprbwVge7nMJ",
        "outputId": "3d0f2eaa-8073-44e8-b7df-bb97403f7394"
      },
      "source": [
        "#这个模块的用意呢：输入单词。输入除了最后一个看，看能不能准确的匹配上最后一位\n",
        "\n",
        "'''\n",
        "1导包\n",
        "2导入文本库 tip：有，直接换行就好了  [字母表]\n",
        "encoding 编码 decoding解码  num_dic 让word 和index 对应起来\n",
        "看下长度26\n",
        "4这才开始导入文本。你想做测试的文本\n",
        "\n",
        "5写函数了，开始要做拆解了(批量处理)\n",
        "目的\n",
        "\n",
        "# Make a batch to have sequence data for input and ouput\n",
        "# wor -> X, d -> Y\n",
        "# dee -> X, p -> Y\n",
        "\n",
        "6开始循环，把除了最后一个【：-1】的字母的index都取出来\n",
        "            # input data is:\n",
        "        # wor       woo       dee     div      ...\n",
        "        # [22, 14, 17] [22, 14, 14] [3, 4, 4] [3, 8, 21] ...\n",
        "\n",
        "\n",
        "    再把最后一个单词取出来，把index 放到target里\n",
        "\n",
        "    #制作one-hot np.eye 批量制作。然后第一个是0.所以index+1的位置是1.代表了这个元素\n",
        "【将输入转为了一键编码】就不像之前我还得一个个编进去用*vec【0】\n",
        " # convert input to one-hot encoding.\n",
        "        # if input is [3, 4, 4]:\n",
        "        # [[ 0,  0,  0,  1,  0,  0,  0, ... 0]\n",
        "        #  [ 0,  0,  0,  0,  1,  0,  0, ... 0]\n",
        "        #  [ 0,  0,  0,  0,  1,  0,  0, ... 0]]\n",
        "'''\n",
        "\n",
        "'''\n",
        "数据准备完开始设置hyerparameters\n",
        "'''\n",
        "\n",
        "'''\n",
        "然后准备dropout 是一个全连接层，开始丢弃神经元了\n",
        "'''"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n然后准备dropout 是一个全连接层，开始丢弃神经元了\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZ6uumwH7kje"
      },
      "source": [
        "#这个过程的数据准备还是很像CBOW的\n",
        "\n",
        "import numpy as np\n",
        "char_arr = ['a','b','c','d','e',\n",
        "            'f','g','h','i','j',\n",
        "            'k','l','m','n','o',\n",
        "            'p','q','r','s','t',\n",
        "            'u','v','w','x','y','z']\n",
        "\n",
        "num_dic = {n:i for i,n in enumerate(char_arr)}  # {'a': 0, 'b': 1, 'c': 2, ..., 'j': 9, 'k', 10, ...}\n",
        "dic_len = len(num_dic)\n",
        "\n",
        "seq_data = ['word','wood','deep','dive','cold',\n",
        "            'cool','load','love','kiss','kind']\n",
        "\n",
        "def make_batch(seq_data):\n",
        "  input_batch = []\n",
        "  target_batch = []\n",
        "\n",
        "  for seq in seq_data:\n",
        "    input_data = [num_dic[n] for n in seq[:-1]] ##不取最后一个，把前面的字母对应的index取出来\n",
        "    target = num_dic[seq[-1]]\n",
        "\n",
        "    input_batch.append(np.eye(dic_len)[input_data])\n",
        "\n",
        "    target_batch.append([target])  #就是输入的index ；target要加【】\n",
        "\n",
        "  return input_batch, target_batch\n",
        "\n",
        "#这个过程的数据准备还是很像CBOW的\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70FqlUfSIyEg"
      },
      "source": [
        "# setting hyperparameters\n",
        "learning_rate = 0.01\n",
        "n_hidden = 64\n",
        "total_epoch = 50\n",
        "\n",
        "n_step = 3    #暂时的RNN序列长度 temporal length of sequences for RNN\n",
        "\n",
        "n_input = dic_len  #26 输入向量的维度（dimension of input vector)\n",
        "n_class = dic_len #类的数量 26 number of class\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYeLmYhpJyaJ"
      },
      "source": [
        "**dropout 让每个隐藏单元更强大； 并推动其自行创建有用的功能。不用依赖其他隐藏单元来纠正错误**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "id": "zh-1aZB1Klci",
        "outputId": "a595b2cd-3cca-41e7-da88-7dbff3be7187"
      },
      "source": [
        "#感觉像是从一个全连接开始丢神经元。（还是全连接）\n",
        "'''\n",
        "1调包\n",
        "建类别 调用nn模型开始建设RNN\n",
        "    讲解RNN中参数： \n",
        "          batch_first 默认（default）False ； 代表输入和输出张量的提供方式为（seq_len, batch_size, feature)\n",
        "          但是：我们需要True ：bec：我们要使用形状输入（batch_size, seq_len, feature)\n",
        "            设置 num_layer = 2 我们将两个RNN 层堆叠在一起形成一个堆叠的RNN。\n",
        "                              第一个RNN接收第一个RNN的输出，并计算最终结果。\n",
        "          应用dropout（=0.2） aim：防止过拟合； 可更改 ； 【注意】可以用在RNN的每一层除了最后一层\n",
        "          https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n",
        "          官方解读\n",
        "\n",
        "  forward function（函数）\n",
        "   nn.RNN 有两个output\n",
        "  1：形状张量（batch_size, seq_len, hidden_size)有点像True；包含每个时间步长t 的RNN的最后一层输出特征。\n",
        "  2： 形状张量（num_layers * num_directions, batch, hidden_size) :包含 t = seq_len 的隐藏状态的张量\n",
        "   我们只关心第一个输出\n",
        "    nn.RNN: https://pytorch.org/docs/stable/generated/torch.nn.RNN.html#torch.nn.RNN\n",
        "\n",
        "\n",
        "    我们仅仅要RNN输出功能中提取最后一个隐藏状态（from output features get(extract) last hidden state)\n",
        "    最后的隐藏状态将信息从RNN信元（cell）随时间流逝中移走\n",
        "    因此    基于最后一个隐藏状态的预测，不仅考虑当前时间'  步长的数据，还要考虑历史数据。（就是输出中包含很多类别的数据）\n",
        "    来源复杂\n",
        "\n",
        "输入然后得到的输出项（forward）\n",
        "\n",
        "  3:设置模型，到GPU\n",
        "  4: loss + optimizer\n",
        "\n",
        "二\n",
        "\n",
        "5模型建设完，开始准备输入数据\n",
        "#https://blog.csdn.net/u014687517/article/details/90770132?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522161698050316780265445973%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&request_id=161698050316780265445973&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_v2~rank_v29-2-90770132.first_rank_v2_pc_rank_v29&utm_term=.view的用途\n",
        "view 用法。\n",
        "view 很像是reshape\n",
        "a = torch.range(1, 25) #a是长度25的张量\n",
        "#改成了5*5：a = a.view(5, 5)\n",
        "或者：a = a.view(-1, 5)\n",
        "\n",
        "（感觉就是将数据类型转变成为了张量。再传参一波，没做什么其他的感觉）\n",
        "\n",
        "三\n",
        "开始训练了epoch\n",
        "然后经典三部 forward + backward + optimize\n",
        "  #神经网络中经常会有\n",
        "  作用:output 是通过神经网络最后一层softmax 函数作用之后的输出。\n",
        "  \n",
        "  例如：假设我们在分类问题中共有4类。 分别是0123； 假如output.data 第一行【1，000】（属于第一个类别的概率是1\n",
        "  不加_, :输出则是1， （其中1是tensor） 加了之后输出变成0\n",
        "\n",
        "  下划线+'，'的作用是：使预测返回的output.data行中最大数值所在位置代表的类别。\n",
        "  _,predicted=torch.max(output.data,dim=1)\n",
        "\n",
        "  【另】\n",
        "    torch.max()返回两个值，一个是具体的value ；用下划线表示。第二个值是 value对应的index（predicted）\n",
        "    下划线_ 表示的就是具体的value，也就是输出的最大值（可用其他名词代替下划线）\n",
        "    数字1其实可以写为dim=1，这里简写为1\n",
        "    dim = 1 输出行的最大值。 dim = 0 输出列的最大值\n",
        "    #https://blog.csdn.net/weixin_48249563/article/details/111387501?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.control&dist_request_id=1328740.37292.16169815268198889&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.control\n",
        "\n",
        "\n",
        "结论是损失和 训练准确度\n",
        "\n",
        "四：\n",
        "预测\n",
        "设置flag 去计算模型； 会 turn off ：dropout \n",
        "将标志设置为评估模式，这将dropout退出。\n",
        "\n",
        "训练完train_datasets之后，model要来测试样本了。在model(test_datasets)之前，需要加上model.eval(). 否则的话，有输入数据，即使不训练，它也会改变权值。这是model中含有batch normalization层所带来的的性质。\n",
        "在做one classification的时候，训练集和测试集的样本分布是不一样的，尤其需要注意这一点。\n",
        "\n",
        "[训练完数据要训练样本；为了让训练集和测试集的样本分开]（感觉更像是实现全连接的需要的感觉）\n",
        "#https://blog.csdn.net/qq_38410428/article/details/101102075?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522161698276116780271557000%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=161698276116780271557000&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-2-101102075.first_rank_v2_pc_rank_v29&utm_term=model.eval\n",
        "\n",
        "先把预测值取出来。\n",
        "然后循环起来\n",
        "\n",
        "pytorch中GPU与CPU的相互转化\n",
        "\n",
        "深度学习中我们默认使用的是CPU，如果我们要使用GPU，需要使用.cuda将计算或者数据从CPU移动至GPU，\n",
        "\n",
        "如果当我们需要在CPU上进行运算时，比如使用plt可视化绘图, 我们可以使用.cpu将计算或者数据转移至CPU.\n",
        "\n",
        "然后开始拼接单词。\n",
        "看拼出来的单词对不对\n",
        "\n",
        "'''"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n1调包\\n建类别 调用nn模型开始建设RNN\\n    讲解RNN中参数： \\n          batch_first 默认（default）False ； 代表输入和输出张量的提供方式为（seq_len, batch_size, feature)\\n          但是：我们需要True ：bec：我们要使用形状输入（batch_size, seq_len, feature)\\n            设置 num_layer = 2 我们将两个RNN 层堆叠在一起形成一个堆叠的RNN。\\n                              第一个RNN接收第一个RNN的输出，并计算最终结果。\\n          应用dropout（=0.2） aim：防止过拟合； 可更改 ； 【注意】可以用在RNN的每一层除了最后一层\\n          https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\\n          官方解读\\n\\n  forward function（函数）\\n   nn.RNN 有两个output\\n  1：形状张量（batch_size, seq_len, hidden_size)有点像True；包含每个时间步长t 的RNN的最后一层输出特征。\\n  2： 形状张量（num_layers * num_directions, batch, hidden_size) :包含 t = seq_len 的隐藏状态的张量\\n   我们只关心第一个输出\\n    nn.RNN: https://pytorch.org/docs/stable/generated/torch.nn.RNN.html#torch.nn.RNN\\n\\n\\n    我们仅仅要RNN输出功能中提取最后一个隐藏状态（from output features get(extract) last hidden state)\\n    最后的隐藏状态将信息从RNN信元（cell）随时间流逝中移走\\n    因此    基于最后一个隐藏状态的预测，不仅考虑当前时间'  步长的数据，还要考虑历史数据。（就是输出中包含很多类别的数据）\\n    来源复杂\\n\\n输入然后得到的输出项（forward）\\n\\n  3:设置模型，到GPU\\n  4: loss + optimizer\\n\\n二\\n\\n5模型建设完，开始准备输入数据\\n#https://blog.csdn.net/u014687517/article/details/90770132?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522161698050316780265445973%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&request_id=161698050316780265445973&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_v2~rank_v29-2-90770132.first_rank_v2_pc_rank_v29&utm_term=.view的用途\\nview 用法。\\nview 很像是reshape\\na = torch.range(1, 25) #a是长度25的张量\\n#改成了5*5：a = a.view(5, 5)\\n或者：a = a.view(-1, 5)\\n\\n（感觉就是将数据类型转变成为了张量。再传参一波，没做什么其他的感觉）\\n\\n三\\n开始训练了epoch\\n然后经典三部 forward + backward + optimize\\n  #神经网络中经常会有\\n  作用:output 是通过神经网络最后一层softmax 函数作用之后的输出。\\n  \\n  例如：假设我们在分类问题中共有4类。 分别是0123； 假如output.data 第一行【1，000】（属于第一个类别的概率是1\\n  不加_, :输出则是1， （其中1是tensor） 加了之后输出变成0\\n\\n  下划线+'，'的作用是：使预测返回的output.data行中最大数值所在位置代表的类别。\\n  _,predicted=torch.max(output.data,dim=1)\\n\\n  【另】\\n    torch.max()返回两个值，一个是具体的value ；用下划线表示。第二个值是 value对应的index（predicted）\\n    下划线_ 表示的就是具体的value，也就是输出的最大值（可用其他名词代替下划线）\\n    数字1其实可以写为dim=1，这里简写为1\\n    dim = 1 输出行的最大值。 dim = 0 输出列的最大值\\n    #https://blog.csdn.net/weixin_48249563/article/details/111387501?utm_medium=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.control&dist_request_id=1328740.37292.16169815268198889&depth_1-utm_source=distribute.pc_relevant.none-task-blog-BlogCommendFromMachineLearnPai2-1.control\\n\\n\\n结论是损失和 训练准确度\\n\\n四：\\n预测\\n设置flag 去计算模型； 会 turn off ：dropout \\n将标志设置为评估模式，这将dropout退出。\\n\\n训练完train_datasets之后，model要来测试样本了。在model(test_datasets)之前，需要加上model.eval(). 否则的话，有输入数据，即使不训练，它也会改变权值。这是model中含有batch normalization层所带来的的性质。\\n在做one classification的时候，训练集和测试集的样本分布是不一样的，尤其需要注意这一点。\\n\\n[训练完数据要训练样本；为了让训练集和测试集的样本分开]（感觉更像是实现全连接的需要的感觉）\\n#https://blog.csdn.net/qq_38410428/article/details/101102075?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522161698276116780271557000%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=161698276116780271557000&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-2-101102075.first_rank_v2_pc_rank_v29&utm_term=model.eval\\n\\n先把预测值取出来。\\n然后循环起来\\n\\npytorch中GPU与CPU的相互转化\\n\\n深度学习中我们默认使用的是CPU，如果我们要使用GPU，需要使用.cuda将计算或者数据从CPU移动至GPU，\\n\\n如果当我们需要在CPU上进行运算时，比如使用plt可视化绘图, 我们可以使用.cpu将计算或者数据转移至CPU.\\n\\n然后开始拼接单词。\\n看拼出来的单词对不对\\n\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyuJ9JTqKygW"
      },
      "source": [
        "#Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cyq3VaAGK1ET",
        "outputId": "a4ebc54c-e70c-4f5a-94be-04b5145939af"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score\n",
        "#感觉固定住了\n",
        "\n",
        "class RNN_Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(RNN_Model, self).__init__()\n",
        "    self.rnn = nn.RNN(n_input, n_hidden, num_layers=2, batch_first=True, dropout=0.2)\n",
        "    self.linear = nn.Linear(n_hidden, n_class) #output的线性层\n",
        "\n",
        "  def forward(self, x):\n",
        "    rnn_output, h_n = self.rnn(x)\n",
        "    x_last = rnn_output[:,-1,:]\n",
        "    x = self.linear(x_last)\n",
        "    return x\n",
        "\n",
        "model = RNN_Model().to(device)\n",
        "criterion = nn.CrossEntropyLoss() \n",
        "optimizer = optim.Adam(model.parameters(), lr= learning_rate)\n",
        "#模型建立好了\n",
        "#然后准备输入\n",
        "input_batch, target_batch = make_batch(seq_data)\n",
        "input_batch_torch = torch.from_numpy(np.array(input_batch)).float().to(device)  #把一个单词前三的字母的one-hot 张量形式放到一个list里面\n",
        "target_batch_torch = torch.from_numpy(np.array(target_batch)).view(-1).to(device) #把每个单元都整合成了长单元\n",
        "# to(device) 将输入转换为张量，并将其设置成为GPU \n",
        "# 参数-1 含义：自行计算行数或列数 \n",
        "\n",
        "\n",
        "for epoch in range(total_epoch):\n",
        "  model.train()  #将标志设置为训练模式；\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  outputs = model(input_batch_torch)    #output训练\n",
        "  loss = criterion(outputs, target_batch_torch)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  _, predicted = torch.max(outputs,1) #这个_代表的是什么\n",
        "\n",
        "  acc = accuracy_score(predicted.cpu().numpy(),target_batch_torch.cpu().numpy()) #50个数据 epoch\n",
        "  print('Epoch: %d, loss: %.5f, train_acc: %.2f' %(epoch+1, loss.item(),acc))\n",
        "\n",
        "print('Finished Training')\n",
        "\n",
        "##Prediction\n",
        "model.eval()\n",
        "outputs = model(input_batch_torch)\n",
        "_, predicted = torch.max(outputs, 1)    # same as predicted = torch.argmax(pred_outputs, 1)\n",
        "\n",
        "predict_words = []\n",
        "for i in range(len(predicted.cpu().numpy())):\n",
        "  ind = predicted.cpu().numpy()[i] #这步骤是让他变回数组而不是张量然后分别取出预测的每个值' index\n",
        "  predict_words.append(seq_data[i][:-1]+char_arr[ind])  #他是去你除了最后一位的单词组合，然后将你预测的index放到对应的字母表中。从字母表中找到真实值。再将两个拼接\n",
        "  print(predict_words)\n",
        "\n",
        "print('\\n=== Prediction Result ===')\n",
        "print('Input:', [w[:3] + ' ' for w in seq_data])\n",
        "print('Predicted:', predict_words)\n",
        "print('Accuracy: %.2f' %acc)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1, loss: 3.19482, train_acc: 0.00\n",
            "Epoch: 2, loss: 2.61596, train_acc: 0.50\n",
            "Epoch: 3, loss: 1.93472, train_acc: 0.50\n",
            "Epoch: 4, loss: 1.57602, train_acc: 0.50\n",
            "Epoch: 5, loss: 1.41090, train_acc: 0.50\n",
            "Epoch: 6, loss: 1.28457, train_acc: 0.50\n",
            "Epoch: 7, loss: 1.34090, train_acc: 0.60\n",
            "Epoch: 8, loss: 1.30429, train_acc: 0.60\n",
            "Epoch: 9, loss: 1.24063, train_acc: 0.50\n",
            "Epoch: 10, loss: 1.05597, train_acc: 0.70\n",
            "Epoch: 11, loss: 0.96480, train_acc: 0.70\n",
            "Epoch: 12, loss: 0.83078, train_acc: 0.70\n",
            "Epoch: 13, loss: 0.85647, train_acc: 0.60\n",
            "Epoch: 14, loss: 0.67300, train_acc: 0.70\n",
            "Epoch: 15, loss: 0.59613, train_acc: 0.80\n",
            "Epoch: 16, loss: 0.54070, train_acc: 0.80\n",
            "Epoch: 17, loss: 0.42144, train_acc: 0.90\n",
            "Epoch: 18, loss: 0.36935, train_acc: 0.90\n",
            "Epoch: 19, loss: 0.33272, train_acc: 0.90\n",
            "Epoch: 20, loss: 0.28066, train_acc: 1.00\n",
            "Epoch: 21, loss: 0.24454, train_acc: 1.00\n",
            "Epoch: 22, loss: 0.22362, train_acc: 1.00\n",
            "Epoch: 23, loss: 0.20707, train_acc: 1.00\n",
            "Epoch: 24, loss: 0.19545, train_acc: 1.00\n",
            "Epoch: 25, loss: 0.17287, train_acc: 1.00\n",
            "Epoch: 26, loss: 0.14037, train_acc: 1.00\n",
            "Epoch: 27, loss: 0.13258, train_acc: 1.00\n",
            "Epoch: 28, loss: 0.16831, train_acc: 0.90\n",
            "Epoch: 29, loss: 0.07802, train_acc: 1.00\n",
            "Epoch: 30, loss: 0.08974, train_acc: 1.00\n",
            "Epoch: 31, loss: 0.09181, train_acc: 1.00\n",
            "Epoch: 32, loss: 0.05433, train_acc: 1.00\n",
            "Epoch: 33, loss: 0.08483, train_acc: 1.00\n",
            "Epoch: 34, loss: 0.03461, train_acc: 1.00\n",
            "Epoch: 35, loss: 0.02319, train_acc: 1.00\n",
            "Epoch: 36, loss: 0.03126, train_acc: 1.00\n",
            "Epoch: 37, loss: 0.01615, train_acc: 1.00\n",
            "Epoch: 38, loss: 0.02997, train_acc: 1.00\n",
            "Epoch: 39, loss: 0.02518, train_acc: 1.00\n",
            "Epoch: 40, loss: 0.01512, train_acc: 1.00\n",
            "Epoch: 41, loss: 0.04558, train_acc: 1.00\n",
            "Epoch: 42, loss: 0.00724, train_acc: 1.00\n",
            "Epoch: 43, loss: 0.00849, train_acc: 1.00\n",
            "Epoch: 44, loss: 0.00681, train_acc: 1.00\n",
            "Epoch: 45, loss: 0.01771, train_acc: 1.00\n",
            "Epoch: 46, loss: 0.00615, train_acc: 1.00\n",
            "Epoch: 47, loss: 0.00937, train_acc: 1.00\n",
            "Epoch: 48, loss: 0.00599, train_acc: 1.00\n",
            "Epoch: 49, loss: 0.00325, train_acc: 1.00\n",
            "Epoch: 50, loss: 0.00300, train_acc: 1.00\n",
            "Finished Training\n",
            "['word']\n",
            "['word', 'wood']\n",
            "['word', 'wood', 'deep']\n",
            "['word', 'wood', 'deep', 'dive']\n",
            "['word', 'wood', 'deep', 'dive', 'cold']\n",
            "['word', 'wood', 'deep', 'dive', 'cold', 'cool']\n",
            "['word', 'wood', 'deep', 'dive', 'cold', 'cool', 'load']\n",
            "['word', 'wood', 'deep', 'dive', 'cold', 'cool', 'load', 'love']\n",
            "['word', 'wood', 'deep', 'dive', 'cold', 'cool', 'load', 'love', 'kiss']\n",
            "['word', 'wood', 'deep', 'dive', 'cold', 'cool', 'load', 'love', 'kiss', 'kind']\n",
            "\n",
            "=== Prediction Result ===\n",
            "Input: ['wor ', 'woo ', 'dee ', 'div ', 'col ', 'coo ', 'loa ', 'lov ', 'kis ', 'kin ']\n",
            "Predicted: ['word', 'wood', 'deep', 'dive', 'cold', 'cool', 'load', 'love', 'kiss', 'kind']\n",
            "Accuracy: 1.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97H71hv5K4Lj"
      },
      "source": [
        "#**Seq2Seq Model(N to M)**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "MyY9ABm600ZK",
        "outputId": "91a310a6-04fe-464e-9871-41423b954892"
      },
      "source": [
        "'''\n",
        "将一个序列转换为另一个序列。用RNN或者常用LSTM 或者GRU去避免梯度消失的问题。每个项目的上下文是上一步的输出。主要组件是一个编码器和一个解码器网络。\n",
        "编码器将每个项目转换为包含该项目及其上下文的隐藏向量。\n",
        "解码器使用先前的输出作为上下文来逆转该过程。将向量转换为输出项\n",
        "'''"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n将一个序列转换为另一个序列。用RNN或者常用LSTM 或者GRU去避免梯度消失的问题。每个项目的上下文是上一步的输出。主要组件是一个编码器和一个解码器网络。\\n编码器将每个项目转换为包含该项目及其上下文的隐藏向量。\\n解码器使用先前的输出作为上下文来逆转该过程。将向量转换为输出项\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYHs3bzrblSZ"
      },
      "source": [
        "**要开始一个新的项目了，该模型将纸牌符号（Ace，Jack，Queen，King） 转换为其相关编号**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "oQrvVGVIb6Ps",
        "outputId": "7501e540-c727-4439-9214-a340cc067f66"
      },
      "source": [
        "'''\n",
        "其实就两个模型一个是RNN，一个是Seq2Seq\n",
        "1：处理数据。\n",
        "2分批次\n",
        "3模型\n",
        "4评估\n",
        "\n",
        "这个任务是\"ace\" 转变到\"01\" 将\"jack\"转变到\"11\"  【要求】老师\n",
        "'''"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n其实就两个模型一个是RNN，一个是Seq2Seq\\n1：处理数据。\\n2分批次\\n3模型\\n4评估\\n\\n这个任务是\"ace\" 转变到\"01\" 将\"jack\"转变到\"11\"  【要求】老师\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJdN7qsdcWxs"
      },
      "source": [
        "**Preprocess data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "9V96X1H6cekP",
        "outputId": "831f0cb3-ff30-4265-a729-eb09bb4e9769"
      },
      "source": [
        "'''\n",
        "1调包\n",
        "2 序列数据 sequence data\n",
        "3 chars 字符  生成唯一的符号列表     tokens符号 \n",
        "  每一个都拆开 + 去掉重复项\n",
        "4 需要特殊的tokens \n",
        "    BEPU\n",
        "    要额外添加\n",
        "    B 序列开始\n",
        "    E 序列结束\n",
        "    P padding 填充（fill） 序列。 对于不同的输入大小\n",
        "    U 序列中不知道的元素。 对于不同的输入大小\n",
        "\n",
        "5 然后将字母和序号一一匹配上 ： n：i\n",
        "6 记录整个长度\n",
        "7 限定最大输入和输出的数量\n",
        "'''"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n1调包\\n2 序列数据 sequence data\\n3 chars 字符  生成唯一的符号列表     tokens符号 \\n  每一个都拆开 + 去掉重复项\\n4 需要特殊的tokens \\n    BEPU\\n    要额外添加\\n    B 序列开始\\n    E 序列结束\\n    P padding 填充（fill） 序列。 对于不同的输入大小\\n    U 序列中不知道的元素。 对于不同的输入大小\\n\\n5 然后将字母和序号一一匹配上 ： n：i\\n6 记录整个长度\\n7 限定最大输入和输出的数量\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c51nQLK-cdH7"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pprint\n",
        "\n",
        "seq_data = [['ace', '01'],['jack','11'],\n",
        "            ['queen','12'],['king','13']]\n",
        "\n",
        "chars =[] \n",
        "for seq in seq_data:\n",
        "  chars += list(seq[0])\n",
        "  chars += list(seq[1])\n",
        "\n",
        "char_arr = list(set(chars)) #筛除重复的然后生成了列表 14\n",
        "\n",
        "char_arr.append('B')\n",
        "char_arr.append('E')\n",
        "char_arr.append('P')\n",
        "char_arr.append('U')    #18\n",
        "\n",
        "\n",
        "num_dic = {n:i for i,n in enumerate(char_arr)} #对应字母：index 因为有0所以17 enumerate 就是让字母和数字一一对应\n",
        "\n",
        "dic_len = len(num_dic)    #18\n",
        "\n",
        "max_input_words_amount = 5\n",
        "max_output_words_amount = 3"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rIDCWd1hTTg"
      },
      "source": [
        "**产生batch--generate batch**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4p8wOeV57hB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "f7c88e8d-eed8-4d7c-909a-5a74c39b877a"
      },
      "source": [
        "'''\n",
        "1.padding (fill 补充) 添满如果单词比最长的单词要短\n",
        "2.产生一批数据，给训练和测试\n",
        "    创建空白列表；编码输入；解码输入； 目标批次\n",
        "    然后开始输入，将字典的中的word输入到input中\n",
        " 然后找到固定的每个单词对应的index列表 P就是16\n",
        "\n",
        "[5, 1, 0, 16, 16]\n",
        "[11, 5, 1, 4, 16]\n",
        "[8, 2, 0, 0, 3]\n",
        "[4, 7, 3, 6, 16]\n",
        "\n",
        "    左边的输入准备完了，然后要进入右边了，要先来个B  B + index\n",
        "[14, 13, 9]\n",
        "[14, 9, 9]\n",
        "[14, 9, 10]\n",
        "[14, 9, 12]\n",
        "\n",
        "    B将index也转换成列表了\n",
        "    但之后就取消了B，加了E 来让index1又转换成列表\n",
        "[13, 9, 15]\n",
        "[9, 9, 15]\n",
        "[9, 10, 15]\n",
        "[9, 12, 15]   \n",
        "  \n",
        "  所以右边是解码器；\n",
        "    解码器单元的输出（实际结果），在序列数据的末尾添加E\n",
        "    由于目标长度是固定的，但实际上我们  不需要在最后加E\n",
        "    但不确定的目标长度，E就很重要了。\n",
        "    因为在预测的过程中当涉及到E我们就要停\n",
        "\n",
        "   然后开始将准备好的编码和解码输入变成 one-hot (逃不掉的one-hot)\n",
        "\n",
        "3 这里输入的编码和解码（b） 都变成了one-hot存起来了\n",
        "只有target （e） 存的是index，并没有存one-hot\n",
        "'''\n",
        "\n",
        "#这里是把词表输入。然后呢处理他们，得到 解码编码-onehot ；和目标（可能就是真实）index"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n1.padding (fill 补充) 添满如果单词比最长的单词要短\\n2.产生一批数据，给训练和测试\\n    创建空白列表；编码输入；解码输入； 目标批次\\n    然后开始输入，将字典的中的word输入到input中\\n 然后找到固定的每个单词对应的index列表 P就是16\\n\\n[5, 1, 0, 16, 16]\\n[11, 5, 1, 4, 16]\\n[8, 2, 0, 0, 3]\\n[4, 7, 3, 6, 16]\\n\\n    左边的输入准备完了，然后要进入右边了，要先来个B  B + index\\n[14, 13, 9]\\n[14, 9, 9]\\n[14, 9, 10]\\n[14, 9, 12]\\n\\n    B将index也转换成列表了\\n    但之后就取消了B，加了E 来让index1又转换成列表\\n[13, 9, 15]\\n[9, 9, 15]\\n[9, 10, 15]\\n[9, 12, 15]   \\n  \\n  所以右边是解码器；\\n    解码器单元的输出（实际结果），在序列数据的末尾添加E\\n    由于目标长度是固定的，但实际上我们  不需要在最后加E\\n    但不确定的目标长度，E就很重要了。\\n    因为在预测的过程中当涉及到E我们就要停\\n\\n   然后开始将准备好的编码和解码输入变成 one-hot (逃不掉的one-hot)\\n\\n3 这里输入的编码和解码（b） 都变成了one-hot存起来了\\n只有target （e） 存的是index，并没有存one-hot\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0O9LFDDUhYfM"
      },
      "source": [
        "def add_paddings(word, max_num=5):    #padding (fill 补充) 添满如果单词比最长的单词要短\n",
        "  diff = max_num -len(word)\n",
        "  return word + 'P'*diff    #用P补全  看起来要不补一个要不补两个P\n",
        "\n",
        "def make_batch(seq_data):   #产生一批数据，给训练和测试\n",
        "  encoder_input_batch = []\n",
        "  decoder_input_batch = []\n",
        "  target_batch = []\n",
        "\n",
        "  for seq in seq_data:\n",
        "    input_word = add_paddings(seq[0])     # 0 指前面的word 1 指后面的index  kingP\n",
        "    en_input_data = [num_dic[n] for n in input_word]          #[5, 1, 0, 16, 16]\n",
        "    \n",
        "    de_input_data = [num_dic[n] for n in ('B' + seq[1])]  #B：14， 13：0； 9：1 它是把idnex也变成了列表\n",
        "\n",
        "    target = [num_dic[n] for n in (seq[1] + 'E')]       #   13:0      E:15\n",
        "                                                        #[en_input_data]落了一个这个，后面就都不对了\n",
        "    encoder_input_batch.append(np.eye(dic_len)[en_input_data]) #就是五位。让他们在对应位置表1；  5；8；4；7；17 （还是需要减一，是从0开始；所以单独输出的还是King「 \n",
        "                                                #让他们input的word变成one-hot\n",
        "    decoder_input_batch.append(np.eye(dic_len)[de_input_data])    #np.eye(总单词长度)【实际你想要的变得单词index】\n",
        "                                                #还是包含B的index\n",
        "    target_batch.append(target)                 #包含E的,他是直接将输出的index存起来了，没做one-hot\n",
        "\n",
        "\n",
        "  return encoder_input_batch, decoder_input_batch, target_batch"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJ0vEq3wEz4G"
      },
      "source": [
        "#Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EW180dzE2LU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "b9c68092-c214-4dd0-8464-62e6ba518e2e"
      },
      "source": [
        "#数据处理好了开始做模型了。\n",
        "'''\n",
        "1，导包\n",
        "2，开始键模型nn模型来了\n",
        "    有一层编码  RNN编码器\n",
        "    有一层dropout tf.nn.dropout是TensorFlow里面为了防止或减轻过拟合而使用的函数，它一般用在全连接层。\n",
        "      这里应该是pytorch的 dropout\n",
        "\n",
        "        tf.nn.dropout(x, keep_prob, noise_shape=None, seed=None,name=None)\n",
        "        上面方法中常用的是前两个参数：\n",
        "        第一个参数x：指输入\n",
        "        第二个参数keep_prob: 设置神经元被选中的概率,在初始化时keep_prob是一个占位符, keep_prob = tf.placeholder(tf.float32) 。tensorflow在run时设置keep_prob具体的值，例如keep_prob: 0.5\n",
        "        第五个参数name：指定该操作的名字。\n",
        "      drop out 可以作为一个层，在RNN输出中，请见下面的forward\n",
        "      以前我们将其作为nn.RNN 的参数（dropout= 0.2）\n",
        "\n",
        "      RNN 解码器 就比编码器多了一层线性\n",
        "'''\n",
        "\n",
        "'''\n",
        "开始向前传播了。\n",
        "隐藏层，包含隐藏状态对于 t = seq_len\n",
        "应用 dropout 层在 输出的RNN上\n",
        "\n",
        "设置隐藏作为 rnn解码器的初始状态。【很重要】\n",
        "\n",
        "模型的原理有点混乱需要重新看KDB或者Caren的课程。\n",
        "\n",
        "\n",
        "\n",
        "# prediction_output_before_softmax = self.linear(decoder_output)  也就是我们只需要这部了，因为nn.CrossEntropyLoss 结合了 logsoftmax 和Nllloss\n",
        "# output_after_softmax = torch.log_softmax(prediction_output_before_softmax,dim=-1)\n",
        "# Since nn.CrossEntropyLoss combines LogSoftmax and NLLLoss for us, we only need the prediction_output_before_softmax\n",
        "\n",
        "        '''"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n开始向前传播了。\\n隐藏层，包含隐藏状态对于 t = seq_len\\n应用 dropout 层在 输出的RNN上\\n\\n设置隐藏作为 rnn解码器的初始状态。【很重要】\\n\\n模型的原理有点混乱需要重新看KDB或者Caren的课程。\\n\\n\\n\\n# prediction_output_before_softmax = self.linear(decoder_output)  也就是我们只需要这部了，因为nn.CrossEntropyLoss 结合了 logsoftmax 和Nllloss\\n# output_after_softmax = torch.log_softmax(prediction_output_before_softmax,dim=-1)\\n# Since nn.CrossEntropyLoss combines LogSoftmax and NLLLoss for us, we only need the prediction_output_before_softmax\\n\\n        '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvsvxHjWL11s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c154ceb3-f98b-4015-d3ba-7b6e548a4883"
      },
      "source": [
        "#setting hyperparameters\n",
        "'''\n",
        "设置超参开始训练\n",
        "然后训练\n",
        "'''"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n设置超参开始训练\\n然后训练\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XB5-N76KE66K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc574642-78d1-4f39-f9c6-ac5d978c78f5"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "class Seq2Seq_Model(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Seq2Seq_Model, self).__init__()\n",
        "\n",
        "    self.rnn_encoder = nn.RNN(n_input, n_hidden, batch_first = True)    #RNN编码器 encoder  #self.rnn_encoder 不是self.rnn.encoder\n",
        "    self.dropout_encoder = nn.Dropout(0.1)    #我认为应该是p = 0.1   # dropout 可以作为一个层\n",
        "    \n",
        "                                              #RNN decoder\n",
        "    self.rnn_decoder = nn.RNN(n_input, n_hidden, batch_first = True)\n",
        "    self.dropout_decoder = nn.Dropout(0.1)\n",
        "    self.linear = nn.Linear(n_hidden,n_class) #多了层线性\n",
        "\n",
        "  def forward(self, x_encoder, x_decoder):\n",
        "    _,hidden = self.rnn_encoder(x_encoder)    # 下划线+'，'的作用是：使预测返回的output.data行中最大数值所在位置代表的类别。_,predicted=torch.max(output.data,dim=1)\n",
        "    hidden = self.dropout_encoder(hidden)   #输出上用dropout层\n",
        "\n",
        "                                            #***** 设置隐藏作为 rnn解码器的初始状态。\n",
        "    decoder_output,_ = self.rnn_decoder(x_decoder,hidden)\n",
        "    decoder_output = self.dropout_decoder(decoder_output)      #输出上用dropout层 和上个hidden的功能描述一样\n",
        "\n",
        "    output = self.linear(decoder_output)\n",
        "\n",
        "    return output\n",
        "'''\n",
        "然后要输入超参开始训练了\n",
        "'''\n",
        "\n",
        "learning_rate = 0.01\n",
        "n_hidden = 128\n",
        "total_epoch = 200\n",
        "\n",
        "n_class = dic_len\n",
        "n_input = dic_len\n",
        "\n",
        "model = Seq2Seq_Model().to(device)    ## to(device) 将输入转换为张量，并将其设置成为GPU \n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n",
        "\n",
        "encoder_input_batch, decoder_input_batch, target_batch = make_batch(seq_data) \n",
        "encoder_input_torch = torch.from_numpy(np.array(encoder_input_batch)).float().to(device)  #4位张量目前对角都是1 [4,18,18]\n",
        "decoder_input_torch = torch.from_numpy(np.array(decoder_input_batch)).float().to(device)  #list---array【没跑之前】跑之后，【4，3，18】\n",
        "target_batch_torch = torch.from_numpy(np.array(target_batch)).view(-1).to(device) # (-1)自行计算行数或列数\n",
        "                                                                        #把所有的index都合在一个tensor里了。[12]\n",
        "\n",
        "for epoch in range(total_epoch):      #循环数据集多次\n",
        "  model.train()\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  #forward + backward + optimize\n",
        "  outputs = model(encoder_input_torch,decoder_input_torch)\n",
        "  loss = criterion(outputs.view(-1,outputs.size(-1)), target_batch_torch)   #tensor(2.8513, \n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if epoch%10==9:   #每10个\n",
        "    print('Epoch: %d, loss: %.5f' %(epoch +1, loss.item()))\n",
        "\n",
        "  \n",
        "print('Finished Training')\n",
        "#loss 每次都不同"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 10, loss: 0.65843\n",
            "Epoch: 20, loss: 0.51416\n",
            "Epoch: 30, loss: 0.34315\n",
            "Epoch: 40, loss: 0.32002\n",
            "Epoch: 50, loss: 0.29404\n",
            "Epoch: 60, loss: 0.31863\n",
            "Epoch: 70, loss: 0.16016\n",
            "Epoch: 80, loss: 0.10391\n",
            "Epoch: 90, loss: 0.13731\n",
            "Epoch: 100, loss: 0.07034\n",
            "Epoch: 110, loss: 0.01164\n",
            "Epoch: 120, loss: 0.00538\n",
            "Epoch: 130, loss: 0.00415\n",
            "Epoch: 140, loss: 0.00190\n",
            "Epoch: 150, loss: 0.00120\n",
            "Epoch: 160, loss: 0.00110\n",
            "Epoch: 170, loss: 0.00095\n",
            "Epoch: 180, loss: 0.00056\n",
            "Epoch: 190, loss: 0.00065\n",
            "Epoch: 200, loss: 0.00055\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1O5ThzbaQ5Rv"
      },
      "source": [
        "#Evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnjW_QHuQ86r"
      },
      "source": [
        "#开始评估了\n",
        "'''\n",
        "定义预测的单词\n",
        "  增添填充满\n",
        "  设置每个预测的角色为不知道U\n",
        "  【'king'（padded），'UU'】\n",
        "预测第一个token\n",
        "预测第二个token  标记\n",
        "'''"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aP4cqbiL2ELt",
        "outputId": "aec37260-30f7-4b90-d12c-d3258754c6d6"
      },
      "source": [
        "def predict(word):\n",
        "  word = add_paddings(word, max_input_words_amount)   #所以这里的word传的是谁呢？\n",
        "\n",
        "  seq_data = [word, 'U' * 2]\n",
        "\n",
        "  encoder_input_batch, decoder_input_batch, _ = make_batch([seq_data])\n",
        "  encoder_input_torch = torch.from_numpy(np.array(encoder_input_batch)).float().to(device)\n",
        "  decoder_input_torch = torch.from_numpy(np.array(decoder_input_batch)).float().to(device)\n",
        "\n",
        "  model.eval()\n",
        "  outputs = model(encoder_input_torch, decoder_input_torch)\n",
        "  predicted = torch.argmax(outputs, -1)\n",
        "  first_token = char_arr[predicted.cpu().numpy()[0][0]]\n",
        "  \n",
        "  seq_data[1] = first_token + 'U' #预测第二个标记\n",
        "\n",
        "  encoder_input_batch, decoder_input_batch, _ = make_batch([seq_data])\n",
        "  encoder_input_torch = torch.from_numpy(np.array(encoder_input_batch)).float().to(device)\n",
        "  decoder_input_torch = torch.from_numpy(np.array(decoder_input_batch)).float().to(device)\n",
        "\n",
        "  model.eval()\n",
        "  outputs = model(encoder_input_torch,decoder_input_torch)\n",
        "  predicted = torch.argmax(outputs, -1)\n",
        "  second_token = char_arr[predicted.cpu().numpy()[0][1]]\n",
        "\n",
        "  return first_token + second_token\n",
        "\n",
        "print('=== Prediction result ===')\n",
        "print('ace ->', predict('ace'))\n",
        "print('jack ->', predict('jack'))\n",
        "print('queen ->', predict('queen'))\n",
        "print('king ->', predict('king'))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=== Prediction result ===\n",
            "ace -> 01\n",
            "jack -> 11\n",
            "queen -> 12\n",
            "king -> 13\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TV1JzhXO8oMs"
      },
      "source": [
        "**Text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7spQOFW6F6E"
      },
      "source": [
        "# Task is \"ace\" -> \"01\", \"jack\" -> \"11\",  a sequence to sequence problem with teacher forcing\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Sequence data\n",
        "seq_data = [['ace', '01'], ['jack', '11'],\n",
        "            ['queen', '12'], ['king', '13']]\n",
        "\n",
        "# Generate unique tokens list\n",
        "chars = []\n",
        "for seq in seq_data:    \n",
        "    chars += list(seq[0])\n",
        "    chars += list(seq[1])\n",
        "\n",
        "# To simplify the question, we put all characters (including input and output) into one set\n",
        "char_arr = list(set(chars))\n",
        "\n",
        "# special tokens are required\n",
        "# B: Beginning of Sequence\n",
        "# E: Ending of Sequence\n",
        "# P: Padding of Sequence - for different size input\n",
        "# U: Unknown element of Sequence - for different size input\n",
        "char_arr.append('B')\n",
        "char_arr.append('E')\n",
        "char_arr.append('P')\n",
        "char_arr.append('U')\n",
        "\n",
        "num_dic = {n: i for i, n in enumerate(char_arr)}\n",
        "\n",
        "dic_len = len(num_dic)\n",
        "\n",
        "max_input_words_amount = 5\n",
        "max_output_words_amount = 3"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wzPZAWlP6n5_"
      },
      "source": [
        "# add paddings if the word is shorter than the maximum number of words\n",
        "def add_paddings(word, max_num=5):\n",
        "    diff = max_num - len(word)\n",
        "    return word+'P'*diff\n",
        "    \n",
        "\n",
        "# generate a batch data for training/testing\n",
        "def make_batch(seq_data):\n",
        "    encoder_input_batch = []\n",
        "    decoder_input_batch = []\n",
        "    target_batch = []\n",
        "\n",
        "    for seq in seq_data:\n",
        "        # Input for encoder cell, convert to vector\n",
        "        input_word = add_paddings(seq[0])\n",
        "        en_input_data = [num_dic[n] for n in input_word]\n",
        "        \n",
        "        # Input for decoder cell, Add 'B' at the beginning of the sequence data\n",
        "        de_input_data  = [num_dic[n] for n in ('B'+ seq[1])]\n",
        "        \n",
        "        # Output of decoder cell (Actual result), Add 'E' at the end of the sequence data\n",
        "        # Since the length of target is fixed, actually we don't need to add \"E\" in the end\n",
        "        # However, for uncertain target length, 'E' is important since we need to stop when it comes to \"E\" during the prediction\n",
        "        target = [num_dic[n] for n in (seq[1] + 'E')]\n",
        "\n",
        "        # Convert each character vector to one-hot encoding data\n",
        "        encoder_input_batch.append(np.eye(dic_len)[en_input_data])\n",
        "        decoder_input_batch.append(np.eye(dic_len)[de_input_data])\n",
        "        \n",
        "        target_batch.append(target)\n",
        "        \n",
        "\n",
        "    return encoder_input_batch, decoder_input_batch, target_batch\n",
        "\n",
        "    #【这的问题】"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GBeYIAD6W3T"
      },
      "source": [
        "#recurrent （复发的 蕊·科·run3·忒） sequence （计数，序数） （C 困·嘶） neural（牛肉）\n",
        "#RNN ； seq2seq\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# you can GPU(cuda)/ just CPU"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n76w_TZh2zku",
        "outputId": "93329d58-cd17-433f-bee7-fa7df69246dc"
      },
      "source": [
        "#6.1\n",
        "seq_data"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['ace', '01'], ['jack', '11'], ['queen', '12'], ['king', '13']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlKBd9kSQCkc",
        "outputId": "ec88d527-7795-4a57-fbeb-7c0a6f83d184"
      },
      "source": [
        "#5.6\n",
        "loss = criterion(outputs.view(-1,outputs.size(-1)), target_batch_torch)\n",
        "loss\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.5444, device='cuda:0', grad_fn=<NllLossBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-l0xgPo8P952",
        "outputId": "24c0f510-6bfe-4f9a-db91-44e254f077a9"
      },
      "source": [
        "#5.5\n",
        "target_batch_torch"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([12,  5, 15,  5,  5, 15,  5,  6, 15,  5,  7, 15], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8grbgjUP3I7",
        "outputId": "43dd4796-5781-482f-f735-953a47d6702a"
      },
      "source": [
        "#5.4\n",
        "outputs.size(-1)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzcoQCfmPvcW",
        "outputId": "80dad05f-8a3c-4175-ac1e-38ac92865b51"
      },
      "source": [
        "#5.3\n",
        "outputs.view(-1,outputs.size(-1)) #12"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ -5.3477,  -5.8705,  -5.9985,  -5.6836,  -5.6007,   7.7865,  -0.2808,\n",
              "          -0.0754,  -5.3091,  -6.4393,  -5.2367,  -5.7612,   6.0992,  -6.0102,\n",
              "          -6.4937,  -3.2540,  -6.0402,  -5.6420],\n",
              "        [ -8.1359,  -8.3976,  -8.7912,  -8.4217,  -7.7891,   8.7916,   5.1923,\n",
              "           5.2890,  -7.7396,  -8.4006,  -8.0030,  -8.6032,   1.3184,  -8.2034,\n",
              "          -9.1096,  -4.1776,  -7.7197,  -7.9167],\n",
              "        [ -2.2059,  -2.6412,  -2.8927,  -3.0924,  -2.2640,  -2.4952,   2.9284,\n",
              "           4.2778,  -2.7378,  -3.1537,  -2.4296,  -2.9000,  -1.4073,  -3.9331,\n",
              "          -3.6077,  14.3211,  -2.3606,  -3.0859],\n",
              "        [ -4.5275,  -5.6051,  -4.8683,  -5.2986,  -4.9559,   8.1549,  -0.1552,\n",
              "          -0.3142,  -4.6840,  -6.0729,  -4.4751,  -5.3116,   4.7771,  -5.2472,\n",
              "          -5.6654,  -4.0313,  -5.8738,  -4.7678],\n",
              "        [ -7.5813,  -8.3008,  -8.6082,  -7.9169,  -7.6968,   6.7230,   7.1202,\n",
              "           7.9293,  -7.8273,  -7.4920,  -7.9478,  -8.4173,   0.9290,  -8.1355,\n",
              "          -8.8589,  -2.6555,  -7.7854,  -7.9354],\n",
              "        [ -2.2990,  -3.1521,  -2.8919,  -3.6306,  -2.7176,  -1.6116,   0.9959,\n",
              "           2.9042,  -2.7752,  -3.6394,  -2.5147,  -2.8353,  -0.3541,  -3.4580,\n",
              "          -2.9769,  13.5570,  -2.3234,  -2.9289],\n",
              "        [ -4.7579,  -5.2859,  -5.2952,  -5.0344,  -5.3673,   7.8499,  -0.2627,\n",
              "          -0.2760,  -4.8764,  -5.7640,  -4.6251,  -5.4386,   5.4873,  -5.5046,\n",
              "          -6.0692,  -4.5025,  -5.6638,  -5.1913],\n",
              "        [ -7.3983,  -8.1715,  -8.6071,  -8.0265,  -7.4417,   7.0156,   6.5618,\n",
              "           6.8226,  -7.5184,  -7.7196,  -8.0114,  -7.9054,   1.3834,  -8.0815,\n",
              "          -9.1040,  -2.8048,  -7.7542,  -7.6345],\n",
              "        [ -2.4744,  -3.7774,  -2.9928,  -3.7325,  -2.7708,  -1.6085,   1.2280,\n",
              "           3.3984,  -3.1444,  -3.8205,  -3.0389,  -3.0253,  -0.6017,  -3.9698,\n",
              "          -3.7528,  14.0198,  -3.1543,  -3.3199],\n",
              "        [ -4.7343,  -5.7135,  -5.8429,  -5.3639,  -5.2495,   7.1598,  -0.0973,\n",
              "          -0.3237,  -4.9707,  -6.1457,  -5.0091,  -5.4445,   5.5982,  -5.6037,\n",
              "          -6.1690,  -3.7594,  -5.7639,  -5.3612],\n",
              "        [ -8.9423,  -9.4458,  -9.9063,  -9.2860,  -8.6165,   7.8373,   6.9267,\n",
              "           7.3585,  -9.1322,  -9.2148,  -9.1697,  -9.1778,   1.4470,  -9.2159,\n",
              "         -10.1303,  -2.5608,  -8.8967,  -8.8026],\n",
              "        [ -1.9982,  -2.5857,  -2.5420,  -2.7615,  -2.1076,  -2.4000,   2.6393,\n",
              "           3.8250,  -2.3681,  -2.8588,  -2.1351,  -2.4371,  -1.3901,  -3.3059,\n",
              "          -3.4074,  14.1913,  -2.3672,  -2.6425]], device='cuda:0',\n",
              "       grad_fn=<ViewBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0N0JkDkoPqgY",
        "outputId": "0a1358ba-91e9-4ae3-dde9-b8ec7bcf40a7"
      },
      "source": [
        "#5.2\n",
        "outputs.view(-1)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ -5.3477,  -5.8705,  -5.9985,  -5.6836,  -5.6007,   7.7865,  -0.2808,\n",
              "         -0.0754,  -5.3091,  -6.4393,  -5.2367,  -5.7612,   6.0992,  -6.0102,\n",
              "         -6.4937,  -3.2540,  -6.0402,  -5.6420,  -8.1359,  -8.3976,  -8.7912,\n",
              "         -8.4217,  -7.7891,   8.7916,   5.1923,   5.2890,  -7.7396,  -8.4006,\n",
              "         -8.0030,  -8.6032,   1.3184,  -8.2034,  -9.1096,  -4.1776,  -7.7197,\n",
              "         -7.9167,  -2.2059,  -2.6412,  -2.8927,  -3.0924,  -2.2640,  -2.4952,\n",
              "          2.9284,   4.2778,  -2.7378,  -3.1537,  -2.4296,  -2.9000,  -1.4073,\n",
              "         -3.9331,  -3.6077,  14.3211,  -2.3606,  -3.0859,  -4.5275,  -5.6051,\n",
              "         -4.8683,  -5.2986,  -4.9559,   8.1549,  -0.1552,  -0.3142,  -4.6840,\n",
              "         -6.0729,  -4.4751,  -5.3116,   4.7771,  -5.2472,  -5.6654,  -4.0313,\n",
              "         -5.8738,  -4.7678,  -7.5813,  -8.3008,  -8.6082,  -7.9169,  -7.6968,\n",
              "          6.7230,   7.1202,   7.9293,  -7.8273,  -7.4920,  -7.9478,  -8.4173,\n",
              "          0.9290,  -8.1355,  -8.8589,  -2.6555,  -7.7854,  -7.9354,  -2.2990,\n",
              "         -3.1521,  -2.8919,  -3.6306,  -2.7176,  -1.6116,   0.9959,   2.9042,\n",
              "         -2.7752,  -3.6394,  -2.5147,  -2.8353,  -0.3541,  -3.4580,  -2.9769,\n",
              "         13.5570,  -2.3234,  -2.9289,  -4.7579,  -5.2859,  -5.2952,  -5.0344,\n",
              "         -5.3673,   7.8499,  -0.2627,  -0.2760,  -4.8764,  -5.7640,  -4.6251,\n",
              "         -5.4386,   5.4873,  -5.5046,  -6.0692,  -4.5025,  -5.6638,  -5.1913,\n",
              "         -7.3983,  -8.1715,  -8.6071,  -8.0265,  -7.4417,   7.0156,   6.5618,\n",
              "          6.8226,  -7.5184,  -7.7196,  -8.0114,  -7.9054,   1.3834,  -8.0815,\n",
              "         -9.1040,  -2.8048,  -7.7542,  -7.6345,  -2.4744,  -3.7774,  -2.9928,\n",
              "         -3.7325,  -2.7708,  -1.6085,   1.2280,   3.3984,  -3.1444,  -3.8205,\n",
              "         -3.0389,  -3.0253,  -0.6017,  -3.9698,  -3.7528,  14.0198,  -3.1543,\n",
              "         -3.3199,  -4.7343,  -5.7135,  -5.8429,  -5.3639,  -5.2495,   7.1598,\n",
              "         -0.0973,  -0.3237,  -4.9707,  -6.1457,  -5.0091,  -5.4445,   5.5982,\n",
              "         -5.6037,  -6.1690,  -3.7594,  -5.7639,  -5.3612,  -8.9423,  -9.4458,\n",
              "         -9.9063,  -9.2860,  -8.6165,   7.8373,   6.9267,   7.3585,  -9.1322,\n",
              "         -9.2148,  -9.1697,  -9.1778,   1.4470,  -9.2159, -10.1303,  -2.5608,\n",
              "         -8.8967,  -8.8026,  -1.9982,  -2.5857,  -2.5420,  -2.7615,  -2.1076,\n",
              "         -2.4000,   2.6393,   3.8250,  -2.3681,  -2.8588,  -2.1351,  -2.4371,\n",
              "         -1.3901,  -3.3059,  -3.4074,  14.1913,  -2.3672,  -2.6425],\n",
              "       device='cuda:0', grad_fn=<ViewBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGtXdZSrPQ-n",
        "outputId": "c3c8383d-48a0-475e-a3b8-91c87e91da7e"
      },
      "source": [
        "#5.1\n",
        "outputs = model(encoder_input_torch,decoder_input_torch)\n",
        "outputs"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-3.9173e+00, -5.5697e+00, -5.2363e+00, -5.0532e+00, -5.0746e+00,\n",
              "           7.2907e+00, -1.6714e+00, -1.8430e-01, -4.9302e+00, -5.3972e+00,\n",
              "          -4.6389e+00, -5.1971e+00,  5.6213e+00, -5.6339e+00, -5.5697e+00,\n",
              "          -3.3276e+00, -5.5813e+00, -4.8254e+00],\n",
              "         [-7.6901e+00, -8.1738e+00, -8.5171e+00, -8.0193e+00, -7.9836e+00,\n",
              "           9.1194e+00,  3.4588e+00,  4.1480e+00, -7.4914e+00, -8.3521e+00,\n",
              "          -7.7683e+00, -8.1236e+00,  2.2012e+00, -8.0564e+00, -8.3892e+00,\n",
              "          -3.0812e+00, -7.5897e+00, -7.5448e+00],\n",
              "         [-2.1666e+00, -2.9426e+00, -3.0201e+00, -3.4295e+00, -2.2210e+00,\n",
              "          -2.3405e+00,  1.7433e+00,  3.6801e+00, -2.7634e+00, -3.3306e+00,\n",
              "          -2.7341e+00, -2.8670e+00, -8.2571e-01, -3.5560e+00, -3.1427e+00,\n",
              "           1.4274e+01, -2.2170e+00, -2.7895e+00]],\n",
              "\n",
              "        [[-4.7471e+00, -5.4746e+00, -5.2784e+00, -5.4372e+00, -5.2484e+00,\n",
              "           7.1052e+00,  6.1981e-02, -4.1968e-01, -4.9605e+00, -6.2743e+00,\n",
              "          -4.7935e+00, -5.4785e+00,  6.8325e+00, -6.2184e+00, -5.6894e+00,\n",
              "          -3.6275e+00, -5.8333e+00, -5.2361e+00],\n",
              "         [-8.3614e+00, -8.7300e+00, -9.3145e+00, -8.6438e+00, -8.3631e+00,\n",
              "           7.8993e+00,  7.4749e+00,  6.6362e+00, -8.6172e+00, -8.4214e+00,\n",
              "          -8.3242e+00, -8.9192e+00,  9.5316e-01, -8.3463e+00, -9.2301e+00,\n",
              "          -2.7323e+00, -8.0374e+00, -8.4014e+00],\n",
              "         [-2.1805e+00, -2.8826e+00, -2.8671e+00, -3.2443e+00, -2.7758e+00,\n",
              "          -2.0276e+00,  2.0069e+00,  2.7925e+00, -2.7753e+00, -3.5061e+00,\n",
              "          -2.2199e+00, -2.8499e+00, -5.7750e-01, -3.6395e+00, -3.4135e+00,\n",
              "           1.3960e+01, -2.3383e+00, -2.8856e+00]],\n",
              "\n",
              "        [[-4.7779e+00, -5.5870e+00, -5.1231e+00, -4.9584e+00, -5.0192e+00,\n",
              "           6.8320e+00, -1.3108e-01, -1.2313e-01, -4.8701e+00, -6.0204e+00,\n",
              "          -4.6849e+00, -5.4537e+00,  6.4466e+00, -5.7049e+00, -6.0227e+00,\n",
              "          -3.4786e+00, -5.5819e+00, -5.0145e+00],\n",
              "         [-8.6764e+00, -8.7809e+00, -9.5858e+00, -8.9815e+00, -8.4739e+00,\n",
              "           7.4145e+00,  7.4973e+00,  7.0068e+00, -8.5955e+00, -8.4702e+00,\n",
              "          -8.8641e+00, -8.8964e+00,  1.4920e+00, -8.2421e+00, -9.2240e+00,\n",
              "          -3.1171e+00, -8.5116e+00, -8.3535e+00],\n",
              "         [-2.9685e+00, -3.5514e+00, -3.7106e+00, -4.1279e+00, -3.1017e+00,\n",
              "          -1.7079e+00,  2.9139e+00,  4.7946e+00, -3.8236e+00, -4.0559e+00,\n",
              "          -3.3659e+00, -3.7526e+00, -1.1280e+00, -4.3717e+00, -4.1168e+00,\n",
              "           1.4112e+01, -3.5479e+00, -3.9088e+00]],\n",
              "\n",
              "        [[-4.5418e+00, -5.6505e+00, -5.4182e+00, -5.4138e+00, -5.3791e+00,\n",
              "           7.3525e+00,  2.4472e-01, -5.5245e-01, -4.9299e+00, -5.6244e+00,\n",
              "          -4.9561e+00, -5.5868e+00,  5.5144e+00, -5.5362e+00, -5.7787e+00,\n",
              "          -3.4161e+00, -5.8719e+00, -4.9845e+00],\n",
              "         [-8.9008e+00, -8.8231e+00, -9.7817e+00, -8.5432e+00, -8.1908e+00,\n",
              "           7.1954e+00,  7.2271e+00,  7.3304e+00, -8.3570e+00, -8.6649e+00,\n",
              "          -8.8435e+00, -8.9825e+00,  2.0706e+00, -8.9785e+00, -9.4327e+00,\n",
              "          -2.9690e+00, -8.3655e+00, -8.8632e+00],\n",
              "         [-3.3179e+00, -3.9662e+00, -4.0353e+00, -4.1738e+00, -3.4827e+00,\n",
              "          -1.1565e+00,  2.6387e+00,  3.6934e+00, -3.8986e+00, -4.2238e+00,\n",
              "          -3.4873e+00, -3.6825e+00, -1.2431e-02, -4.5974e+00, -4.2812e+00,\n",
              "           1.4173e+01, -3.2318e+00, -3.9263e+00]]], device='cuda:0',\n",
              "       grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f98OcKLOpaN",
        "outputId": "2d761c74-bc79-4f23-a459-a0396e7b13f0"
      },
      "source": [
        "target_batch_torch.shape"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([12])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaY6qOB1OfT-",
        "outputId": "a2d67b68-b60b-43a1-eeb9-d23794594c2d"
      },
      "source": [
        "target_batch_torch"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([12,  5, 15,  5,  5, 15,  5,  6, 15,  5,  7, 15], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d84yy2mEOcW4",
        "outputId": "35b4fd86-9b5d-4a44-9e23-1419c002339b"
      },
      "source": [
        "target_batch"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[12, 5, 15], [5, 5, 15], [5, 6, 15], [5, 7, 15]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "id": "pLvd9F2cN5J1",
        "outputId": "ffbab87e-cc3a-4388-9af7-3def9fb6f253"
      },
      "source": [
        "decoder_input_batch.shape"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-47dc7d42148c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdecoder_input_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y12msx76Nm76"
      },
      "source": [
        "encoder_input_batch.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVSzaWx3Nhj8"
      },
      "source": [
        "decoder_input_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQjETWhCNPOl"
      },
      "source": [
        "encoder_input_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DY23fK8D6hd"
      },
      "source": [
        "target_batch.append(target)\n",
        "target_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57KG4MQZD0fW"
      },
      "source": [
        "#4.18\n",
        "target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlnjrhJFDV0y"
      },
      "source": [
        "dic_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWVrYDKCDSjJ"
      },
      "source": [
        "np.eye(dic_len)[de_input_data]  #14, 9 ,12"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fS3oRBgkCzXo"
      },
      "source": [
        "en_input_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFB6aOrJBiZ9"
      },
      "source": [
        "#4.14\n",
        "np.eye(dic_len)[en_input_data] #kingP "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_yYzT3zBej4"
      },
      "source": [
        "#4.13\n",
        "np.eye(dic_len)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCojHtInAS4e"
      },
      "source": [
        "#4.12\n",
        "encoder_input_batch = []\n",
        "decoder_input_batch = []\n",
        "target_batch = []\n",
        "\n",
        "for seq in seq_data:\n",
        "  input_word = add_paddings(seq[0])     # 0 指前面的word 1 指后面的index  kingP\n",
        "  en_input_data = [num_dic[n] for n in input_word]          #[5, 1, 0, 16, 16]\n",
        "  \n",
        "  de_input_data = [num_dic[n] for n in ('B' + seq[1])]  #B：14， 13：0； 9：1 它是把idnex也变成了列表\n",
        "\n",
        "  target = [num_dic[n] for n in (seq[1] + 'E')]\n",
        "  print(target)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kJjjJBv-2Fq"
      },
      "source": [
        "#4.11\n",
        "seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cc9KZB40-vf4"
      },
      "source": [
        "#4.10\n",
        "encoder_input_batch = []\n",
        "decoder_input_batch = []\n",
        "target_batch = []\n",
        "\n",
        "for seq in seq_data:\n",
        "  input_word = add_paddings(seq[0])     # 0 指前面的word 1 指后面的index  kingP\n",
        "  en_input_data = [num_dic[n] for n in input_word]\n",
        "  de_input_data = [num_dic[n] for n in ('B' + seq[1])]\n",
        "  print(de_input_data)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HA42PJis9NV1"
      },
      "source": [
        "#4.9\n",
        "encoder_input_batch = []\n",
        "decoder_input_batch = []\n",
        "target_batch = []\n",
        "\n",
        "for seq in seq_data:\n",
        "  input_word = add_paddings(seq[0])     # 0 指前面的word 1 指后面的index  kingP\n",
        "  en_input_data = [num_dic[n] for n in input_word]\n",
        "  print(en_input_data)    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWQZcDww9CgF"
      },
      "source": [
        "#4.8 \n",
        "n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbkgFh6p88rj"
      },
      "source": [
        "#4.7\n",
        "len(num_dic)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlJ9tfWN82Uo"
      },
      "source": [
        "#4.6\n",
        "num_dic[n]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHQ3J8Ja8xzV"
      },
      "source": [
        "#4.5\n",
        "num_dic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHCcxFBp7Ldv"
      },
      "source": [
        "input_word = add_paddings(seq[0])\n",
        "input_word"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4e3EnRP7CbF"
      },
      "source": [
        "seq[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQm3jTGQ670-"
      },
      "source": [
        "#4.2\n",
        "seq[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RQhpk5F6Q_U"
      },
      "source": [
        "#4.1\n",
        "seq_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "079-aKEFeNNi"
      },
      "source": [
        "#3.1\n",
        "chars += list(seq[0])\n",
        "'''\n",
        "['a', 'c', 'e']\n",
        "['a', 'c', 'e', 'j', 'a', 'c', 'k']\n",
        "['a', 'c', 'e', 'j', 'a', 'c', 'k', 'q', 'u', 'e', 'e', 'n']\n",
        "['a', 'c', 'e', 'j', 'a', 'c', 'k', 'q', 'u', 'e', 'e', 'n', 'k', 'i', 'n', 'g']\n",
        "'''\n",
        "chars += list(seq[1])\n",
        "'''\n",
        "['a', 'c', 'e', '0', '1']\n",
        "['a', 'c', 'e', '0', '1', 'j', 'a', 'c', 'k', '1', '1']\n",
        "['a', 'c', 'e', '0', '1', 'j', 'a', 'c', 'k', '1', '1', 'q', 'u', 'e', 'e', 'n', '1', '2']\n",
        "['a', 'c', 'e', '0', '1', 'j', 'a', 'c', 'k', '1', '1', 'q', 'u', 'e', 'e', 'n', '1', '2', 'k', 'i', 'n', 'g', '1', '3']\n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KQD9RkMxGLS"
      },
      "source": [
        "#2.17\n",
        "seq_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K21ZQs_LwRgl"
      },
      "source": [
        "#2.16\n",
        "char_arr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1R66YV-EwOrK"
      },
      "source": [
        "#2.15\n",
        "char_arr[ind]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMSVcVsrv8ja"
      },
      "source": [
        "#2.14\n",
        "'''\n",
        "['wor']\n",
        "['wor', 'woo']\n",
        "['wor', 'woo', 'dee']\n",
        "['wor', 'woo', 'dee', 'div']\n",
        "['wor', 'woo', 'dee', 'div', 'col']\n",
        "['wor', 'woo', 'dee', 'div', 'col', 'coo']\n",
        "['wor', 'woo', 'dee', 'div', 'col', 'coo', 'loa']\n",
        "['wor', 'woo', 'dee', 'div', 'col', 'coo', 'loa', 'lov']\n",
        "['wor', 'woo', 'dee', 'div', 'col', 'coo', 'loa', 'lov', 'kis']\n",
        "['wor', 'woo', 'dee', 'div', 'col', 'coo', 'loa', 'lov', 'kis', 'kin']\n",
        "'''\n",
        "\n",
        "#seq_data[i][:-1]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQUOTXvqvj6O"
      },
      "source": [
        "#2.13\n",
        "ind"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6jg502ctIyy"
      },
      "source": [
        "#2.12\n",
        "predicted.cpu()[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d605TlHwp12o"
      },
      "source": [
        "#2.11\n",
        "predicted.cpu().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LhtZxWM7pxWL"
      },
      "source": [
        "#2.10\n",
        "predicted.cpu()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sY-Ugp7lpt9l"
      },
      "source": [
        "#2.9\n",
        "predicted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ki13GzewpjGJ"
      },
      "source": [
        "#2.8\n",
        "target_batch_torch.cpu().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a8eC7wkpfGJ"
      },
      "source": [
        "#2.7\n",
        "target_batch_torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3359uRUvpaGj"
      },
      "source": [
        "#2.6\n",
        "print(target_batch_torch.cpu())#tensor([ 3,  3, 15,  4,  3, 11,  3,  4, 18,  3])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXrJ-ibplvKo"
      },
      "source": [
        "#2.5\n",
        "  #print(_)    #tensor([0.2457, 0.1858, 0.2913, 0.1972, 0.2256, 0.2471, 0.2616, 0.1699, 0.2519,\n",
        "              #0.2905], device='cuda:0', grad_fn=<MaxBackward0>)  [10]torch.Size([10]) 50+(不会是64吧）)\n",
        "  #print(len(_))\n",
        "  #print(predicted) #tensor([ 4,  4,  4,  4,  4,  4,  4,  4, 10,  4], device='cuda:0')\n",
        "  #print(predicted.shape) #torch.Size([10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvFumYJCkxkX"
      },
      "source": [
        "#2.4\n",
        "target_batch_torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgIsRMPKkuPx"
      },
      "source": [
        "#2.3\n",
        "target_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6HWwjJpkr8S"
      },
      "source": [
        "#2.2\n",
        "model.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPFvrCr3ZBCl"
      },
      "source": [
        "#2.1\n",
        "input_batch_torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "su5l0yYpIUQH"
      },
      "source": [
        "#1.9\n",
        "'''\n",
        "target_batch = []\n",
        "for seq in seq_data:\n",
        "  input_data = [num_dic[n] for n in seq[:-1]] ##不取最后一个，把前面的字母对应的index取出来\n",
        "  target = num_dic[seq[-1]]\n",
        "\n",
        "  input_batch.append(np.eye(dic_len)[input_data])\n",
        "\n",
        "  target_batch.append([target])\n",
        "  print(target_batch)\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bk2KjCTC6f7"
      },
      "source": [
        "#1.8\n",
        "'''\n",
        "import pprint\n",
        "input_batch = []\n",
        "for seq in seq_data:\n",
        "  input_data = [num_dic[n] for n in seq[:-1]] ##不取最后一个，把前面的字母对应的index取出来\n",
        "\n",
        "  input_batch.append(np.eye(dic_len)[input_data]) #2-3【0】【0】 还是个list；还没有形状0 0\n",
        "  \n",
        "\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcnEvNLxD2_h"
      },
      "source": [
        "#np.eye(dic_len)[input_data] #制作one-hot np.eye 批量制作。然后第一个是0.所以index+1的位置是1.代表了这个元素"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eNXUkvDD9Xb"
      },
      "source": [
        "#input_data #kid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d2YzxQJDnFV"
      },
      "source": [
        "np.eye(dic_len)#弄了个对角线1其他都是0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53CyT1-KB-Nl"
      },
      "source": [
        "#1.7\n",
        "'''\n",
        "for seq in seq_data:\n",
        "  target = num_dic[seq[-1]]\n",
        "  print(target)     #3 :d  \n",
        "  '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5QsK7l4BQlm"
      },
      "source": [
        "#1.6\n",
        "'''\n",
        "for seq in seq_data:\n",
        "    input_data = [num_dic[n] for n in seq[:-1]] #不取最后一个\n",
        "    print(input_data)\n",
        "    #w:22 o:14 r:17\n",
        "    '''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USwFA6d1AoTa"
      },
      "source": [
        "#1.5\n",
        "seq_data\n",
        "len(seq_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rt9G3Hjm_pEH"
      },
      "source": [
        "#1.4\n",
        "dic_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wc1qGzRz_RIK"
      },
      "source": [
        "#1.3\n",
        "{n:i for i, n in enumerate(char_arr)} #   n: i == 'a': 0,单词： index 目前看起来只有字典可以这样"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iGHoeOZ8w4C"
      },
      "source": [
        "#1.2\n",
        "for i,n in enumerate(char_arr):\n",
        "  #print(i,)    #index\n",
        "  #print(n)    #word\n",
        "  print(i,n)  #这样是一一对应（横向）"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSKe8G808VK5"
      },
      "source": [
        "#1.1\n",
        "list(enumerate(char_arr))#这没什么变化啊\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4SsWe_Y8XOy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}